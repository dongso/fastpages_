{
  
    
        "post0": {
            "title": "Use pretrained BERT model for classification",
            "content": "Would Lasse recommend this book? . This is the second part of the three part blogpost on my NLP project. In this blogpost I will show you how to use a pretrained BERT model to finetune a model to predict how I would rate a book based on one page. In the first part I showed how to build the dataset. Now I will show you how to use this data to basically build a model from that. First, let&#39;s get our packages. . !pip install transformers !pip install seaborn . from pathlib import Path import numpy as np import pandas as pd import torch import torch.nn as nn from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report import transformers from transformers import BertTokenizer, BertForSequenceClassification # specify GPU device = torch.device(&quot;cuda&quot;) . We then load the data we got from our images in combination with pytesseract. . Load Data . p = Path.cwd() complete_df = pd.read_csv(p/&#39;datasets/text_df.csv&#39;) complete_df.head() . text title rating . 0 war ein schrecklicher Rückfall eingetreten.In ... | gegendenStrich | 1 | . 1 höchst moralischer Akt, die Welt von einem sol... | derSeewolf | 5 | . 2 deutsches Luder nehmen. Und sollten Sie es dan... | ButchersCrossing | 4 | . 3 müssen.»Sie kamen jetzt in die Vorstadt. Die S... | diePest | 2 | . 4 ins Gesicht, wandte sich von ihrem traurigen A... | diePest | 2 | . We only need the text and my rating. We also need to substract 1 from my rating for indexing purposes for the cross entropy loss function. . df = pd.DataFrame({ &#39;label&#39;: complete_df.iloc[:,2]-1, &#39;text&#39;: complete_df.iloc[:,0] }) df.head() . label text . 0 0 | war ein schrecklicher Rückfall eingetreten.In ... | . 1 4 | höchst moralischer Akt, die Welt von einem sol... | . 2 3 | deutsches Luder nehmen. Und sollten Sie es dan... | . 3 1 | müssen.»Sie kamen jetzt in die Vorstadt. Die S... | . 4 1 | ins Gesicht, wandte sich von ihrem traurigen A... | . # Get the lists of sentences and their labels. sentences = df.text.values labels = df.label.values . Bert Tokenizer . My data is in German. Luckily, the awesome huggingface library provides a crazy amount of pretrained models in languages from all over the world. We first need our tokenizer: . PRE_TRAINED_MODEL_NAME = &#39;bert-base-german-cased&#39; # Load the BERT tokenizer tokenizer = torch.hub.load(&#39;huggingface/pytorch-transformers&#39;, &#39;tokenizer&#39;, PRE_TRAINED_MODEL_NAME) # Download vocabulary from S3 and cache. . Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master . Let&#39;s look what the tokenizer does to our text sentences: . # Print the original sentence. print(&#39; Original: &#39;, sentences[0]) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) . Original: war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Literatur als zu jener andern, bei der er einen Platzbeanspruchte, den man ihm verweigerte. Seine Sprache war die des wilden Romantismus, voll gewunde—ner Wendungen und übertriebener Vergleiche, undeigentlich erschien d’Aurévilly wie ein Zuchthengstunter diesen Wallachen, die die ultramontanen StalleDem Herzog kamen diese Betrachtungen heimgelegentlichen Wiederlesen einiger Stellen diesesC L ( ii i „ .. ‚]:„„„„ „a.—näepn alwxxr9rl’iﬂ» Tokenized: [&#39;war&#39;, &#39;ein&#39;, &#39;schreck&#39;, &#39;##licher&#39;, &#39;Rück&#39;, &#39;##fall&#39;, &#39;eingetreten&#39;, &#39;.&#39;, &#39;In&#39;, &#39;dem&#39;, &#39;[UNK]&#39;, &#39;verheiratet&#39;, &#39;##en&#39;, &#39;Priester&#39;, &#39;[UNK]&#39;, &#39;wurde&#39;, &#39;das&#39;, &#39;Lob&#39;, &#39;##Christ&#39;, &#39;##i&#39;, &#39;von&#39;, &#39;Barb&#39;, &#39;##ey&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;gesungen&#39;, &#39;;&#39;, &#39;in&#39;, &#39;[UNK]&#39;, &#39;Les&#39;, &#39;##Di&#39;, &#39;##ab&#39;, &#39;##oli&#39;, &#39;##ques&#39;, &#39;[UNK]&#39;, &#39;hatte&#39;, &#39;sich&#39;, &#39;der&#39;, &#39;Verfasser&#39;, &#39;dem&#39;, &#39;Teufel&#39;, &#39;ergeben&#39;, &#39;,&#39;, &#39;den&#39;, &#39;er&#39;, &#39;pri&#39;, &#39;##es&#39;, &#39;;&#39;, &#39;und&#39;, &#39;jetzt&#39;, &#39;erschien&#39;, &#39;der&#39;, &#39;Sad&#39;, &#39;##ismus&#39;, &#39;,&#39;, &#39;dieser&#39;, &#39;Bast&#39;, &#39;##ard&#39;, &#39;des&#39;, &#39;Kathol&#39;, &#39;##izismus&#39;, &#39;,&#39;, &#39;den&#39;, &#39;die&#39;, &#39;Religion&#39;, &#39;in&#39;, &#39;##allen&#39;, &#39;Formen&#39;, &#39;mit&#39;, &#39;Ex&#39;, &#39;##or&#39;, &#39;##zi&#39;, &#39;##sm&#39;, &#39;##en&#39;, &#39;und&#39;, &#39;Schei&#39;, &#39;##ter&#39;, &#39;##haufen&#39;, &#39;##durch&#39;, &#39;alle&#39;, &#39;Jahrhunderte&#39;, &#39;verfolgt&#39;, &#39;hat&#39;, &#39;.&#39;, &#39;Mit&#39;, &#39;Barb&#39;, &#39;##ey&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;nahm&#39;, &#39;die&#39;, &#39;Serie&#39;, &#39;der&#39;, &#39;rel&#39;, &#39;##i&#39;, &#39;[UNK]&#39;, &#39;g&#39;, &#39;##i&#39;, &#39;##ösen&#39;, &#39;Schriftsteller&#39;, &#39;ein&#39;, &#39;Ende&#39;, &#39;.&#39;, &#39;Eigentlich&#39;, &#39;gehörte&#39;, &#39;dieser&#39;, &#39;Par&#39;, &#39;##ia&#39;, &#39;in&#39;, &#39;jeder&#39;, &#39;Hinsicht&#39;, &#39;mehr&#39;, &#39;zur&#39;, &#39;welt&#39;, &#39;##lichen&#39;, &#39;Literatur&#39;, &#39;als&#39;, &#39;zu&#39;, &#39;jener&#39;, &#39;andern&#39;, &#39;,&#39;, &#39;bei&#39;, &#39;der&#39;, &#39;er&#39;, &#39;einen&#39;, &#39;Platz&#39;, &#39;##be&#39;, &#39;##anspruch&#39;, &#39;##te&#39;, &#39;,&#39;, &#39;den&#39;, &#39;man&#39;, &#39;ihm&#39;, &#39;verweigerte&#39;, &#39;.&#39;, &#39;Seine&#39;, &#39;Sprache&#39;, &#39;war&#39;, &#39;die&#39;, &#39;des&#39;, &#39;wild&#39;, &#39;##en&#39;, &#39;Roman&#39;, &#39;##ti&#39;, &#39;##sm&#39;, &#39;##us&#39;, &#39;,&#39;, &#39;voll&#39;, &#39;gew&#39;, &#39;##unde&#39;, &#39;[UNK]&#39;, &#39;ne&#39;, &#39;##r&#39;, &#39;Wend&#39;, &#39;##ungen&#39;, &#39;und&#39;, &#39;übert&#39;, &#39;##riebene&#39;, &#39;##r&#39;, &#39;Vergleich&#39;, &#39;##e&#39;, &#39;,&#39;, &#39;und&#39;, &#39;##eigent&#39;, &#39;##lich&#39;, &#39;erschien&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;wie&#39;, &#39;ein&#39;, &#39;Zucht&#39;, &#39;##hen&#39;, &#39;##gst&#39;, &#39;##unter&#39;, &#39;diesen&#39;, &#39;Wall&#39;, &#39;##achen&#39;, &#39;,&#39;, &#39;die&#39;, &#39;die&#39;, &#39;u&#39;, &#39;##lt&#39;, &#39;##ram&#39;, &#39;##ont&#39;, &#39;##anen&#39;, &#39;Stall&#39;, &#39;##e&#39;, &#39;##Dem&#39;, &#39;Herzog&#39;, &#39;kamen&#39;, &#39;diese&#39;, &#39;Betrachtung&#39;, &#39;##en&#39;, &#39;heim&#39;, &#39;##gelegen&#39;, &#39;##tlichen&#39;, &#39;Wieder&#39;, &#39;##lesen&#39;, &#39;einiger&#39;, &#39;Stellen&#39;, &#39;dieses&#39;, &#39;##C&#39;, &#39;L&#39;, &#39;(&#39;, &#39;i&#39;, &#39;##i&#39;, &#39;i&#39;, &#39;[UNK]&#39;, &#39;.&#39;, &#39;.&#39;, &#39;[UNK]&#39;, &#39;]&#39;, &#39;:&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;a&#39;, &#39;.&#39;, &#39;[UNK]&#39;, &#39;n&#39;, &#39;##ä&#39;, &#39;##ep&#39;, &#39;##n&#39;, &#39;al&#39;, &#39;##w&#39;, &#39;##xx&#39;, &#39;##r&#39;, &#39;##9&#39;, &#39;##r&#39;, &#39;##l&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;] Token IDs: [185, 39, 21387, 766, 1060, 441, 9387, 26914, 173, 128, 2, 5025, 7, 7335, 2, 192, 93, 10929, 17339, 26899, 88, 18304, 8145, 9, 2, 2, 20397, 26968, 50, 2, 4189, 15845, 228, 13078, 11226, 2, 466, 144, 21, 18241, 128, 18649, 4254, 26918, 86, 67, 22074, 16, 26968, 42, 1868, 3368, 21, 16073, 1500, 26918, 534, 16804, 587, 91, 9032, 20438, 26918, 86, 30, 9373, 50, 2700, 7685, 114, 1108, 34, 517, 6694, 7, 42, 11168, 60, 26128, 4912, 987, 16902, 7547, 193, 26914, 304, 18304, 8145, 9, 2, 2, 1995, 30, 4345, 21, 4628, 26899, 2, 111, 26899, 3670, 6425, 39, 926, 26914, 13935, 2374, 534, 1059, 544, 50, 2617, 8110, 380, 252, 3522, 248, 3595, 153, 81, 8310, 19919, 26918, 178, 21, 67, 303, 1361, 165, 4465, 26, 26918, 86, 478, 787, 26792, 26914, 2072, 4247, 185, 30, 91, 24703, 7, 3529, 15099, 6694, 51, 26918, 1352, 397, 1270, 2, 2055, 26900, 16380, 184, 42, 8685, 25630, 26900, 3115, 26897, 26918, 42, 7656, 68, 3368, 9, 2, 2, 246, 39, 17373, 215, 22336, 940, 1377, 5405, 794, 26918, 30, 30, 2118, 362, 1021, 710, 6678, 16993, 26897, 12939, 5996, 3484, 620, 12115, 7, 6488, 10547, 5323, 2261, 18921, 7844, 4812, 1328, 26958, 94, 26954, 46, 26899, 46, 2, 26914, 26914, 2, 26985, 26964, 2, 2, 2, 2, 2, 18, 26914, 2, 53, 26923, 3154, 26898, 1119, 26915, 21591, 26900, 26942, 26900, 26907, 2, 2, 2] . So our bert-base-german-cased tokenizer splits the words into reasonable parts, which correspond to the token ids (input ids). . Tokenize Dataset . Let&#39;s check the length for each sequence and print the max sequence length. . max_len = 0 # For every sentence... for sent in sentences: # Tokenize the text and add `[CLS]` and `[SEP]` tokens. input_ids = tokenizer.encode(sent, add_special_tokens=True) # Update the maximum sentence length. max_len = max(max_len, len(input_ids)) print(&#39;Max sentence length: &#39;, max_len) . Token indices sequence length is longer than the specified maximum sequence length for this model (538 &gt; 512). Running this sequence through the model will result in indexing errors . Max sentence length: 538 . Das Maximum sentence length is 538. However, the maximum sentence length allowed by Bert is 512, so we have to set max_len to 256. Next, we tokenize all of our sentences. . # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] attention_masks = [] # For every sentence... for sent in sentences: # `encode_plus` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. # (5) Pad or truncate the sentence to `max_length` # (6) Create attention masks for [PAD] tokens. encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; truncation=True, max_length = 256, # Pad &amp; truncate all sentences. pad_to_max_length = True, #padding=&#39;max_length=256&#39;, return_attention_mask = True, # Construct attn. masks. return_tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding). attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.cat(input_ids, dim=0) attention_masks = torch.cat(attention_masks, dim=0) labels = torch.tensor(labels) . /opt/conda/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). warnings.warn( . Training and Validation Set . We then put our data into a pytorch dataset and split the data into training and validation set. . from torch.utils.data import TensorDataset, random_split # Combine the training inputs into a TensorDataset. dataset = TensorDataset(input_ids, attention_masks, labels) # Create a 80-20 train-validation split. # Calculate the number of samples to include in each set. train_size = int(0.8 * len(dataset)) val_size = len(dataset) - train_size # Divide the dataset by randomly selecting samples. train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) print(&#39;{:&gt;5,} training samples&#39;.format(train_size)) print(&#39;{:&gt;5,} validation samples&#39;.format(val_size)) . 75 training samples 19 validation samples . from torch.utils.data import DataLoader, RandomSampler, SequentialSampler # We take a batch size of 16 batch_size = 16 # We&#39;ll take training samples in random order. train_dataloader = DataLoader( train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size ) # For validation the order doesn&#39;t matter, so we&#39;ll just read them sequentially. validation_dataloader = DataLoader( val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size ) . After putting the dataset into a dataloader we define how many different classes we&#39;ve got. . n_classes=5 . Get pretrained model . model = BertForSequenceClassification.from_pretrained( &quot;bert-base-german-cased&quot;, # Use the 12-layer BERT model, with an uncased vocab. num_labels = n_classes, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) # Tell pytorch to run this model on the GPU. model.cuda(); . Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . Optimizer and Learning Rate . We use AdamW as our optimizer and use CrossEntropyLoss as our loss function. . from transformers import AdamW . # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the &#39;W&#39; stands for &#39;Weight Decay fix&quot; optimizer = AdamW(model.parameters(), lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) . from transformers import get_linear_schedule_with_warmup epochs = 5 # Total number of training steps is [number of batches] x [number of epochs]. # (Note that this is not the same as the number of training samples). total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 10, # Default value in run_glue.py num_training_steps = total_steps) . loss_fn = nn.CrossEntropyLoss().to(device) . def format_time(elapsed): &#39;&#39;&#39; Takes a time in seconds and returns a string hh:mm:ss &#39;&#39;&#39; # Round to the nearest second. elapsed_rounded = int(round((elapsed))) # Format as hh:mm:ss return str(datetime.timedelta(seconds=elapsed_rounded)) . Train Model . Let&#39;s train our model! . import random import numpy as np import time import datetime seed_val = 42 random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) # We&#39;ll store a number of quantities such as training and validation loss, # validation accuracy, and timings. training_stats = [] # Measure the total training time for the whole run. total_t0 = time.time() # For each epoch... for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(&quot;&quot;) print(&#39;======== Epoch {:} / {:} ========&#39;.format(epoch_i + 1, epochs)) print(&#39;Training...&#39;) # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_train_loss = 0 model.train() # For each batch of training data... for step, batch in enumerate(train_dataloader): # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calculate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(f&#39;Batch:{step} of {len(train_dataloader)}. Elapsed: {elapsed}&#39;) # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) model.zero_grad() loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) total_train_loss += loss.item() loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the &quot;exploding gradients&quot; problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over all of the batches. avg_train_loss = total_train_loss / len(train_dataloader) # Measure how long this epoch took. training_time = format_time(time.time() - t0) print(&quot;&quot;) print(&quot; Average training loss: {0:.2f}&quot;.format(avg_train_loss)) print(&quot; Training epoch took: {:}&quot;.format(training_time)) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on # our validation set. print(&quot;&quot;) print(&quot;Running Validation...&quot;) t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model.eval() # Tracking variables total_eval_accuracy = 0 total_eval_loss = 0 nb_eval_steps = 0 # Evaluate data for one epoch for batch in validation_dataloader: b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # Accumulate the validation loss. total_eval_loss += loss.item() # Calculate the average loss over all of the batches. avg_val_loss = total_eval_loss / len(validation_dataloader) # Measure how long the validation run took. validation_time = format_time(time.time() - t0) print(f&#39;Validation Loss: {avg_val_loss}&#39;) print(f&#39;Validation took: {validation_time}&#39;) # Record all statistics from this epoch. training_stats.append( { &#39;epoch&#39;: epoch_i + 1, &#39;Training Loss&#39;: avg_train_loss, &#39;Valid. Loss&#39;: avg_val_loss, &#39;Training Time&#39;: training_time, &#39;Validation Time&#39;: validation_time } ) print(&quot;&quot;) print(&quot;Training complete!&quot;) print(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time()-total_t0))) . ======== Epoch 1 / 5 ======== Training... Average training loss: 1.52 Training epoch took: 0:00:07 Running Validation... Validation Loss: 1.3076387345790863 Validation took: 0:00:01 ======== Epoch 2 / 5 ======== Training... Average training loss: 1.34 Training epoch took: 0:00:07 Running Validation... Validation Loss: 1.1822895407676697 Validation took: 0:00:01 ======== Epoch 3 / 5 ======== Training... Average training loss: 0.85 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.8835422396659851 Validation took: 0:00:01 ======== Epoch 4 / 5 ======== Training... Average training loss: 0.32 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.46347731351852417 Validation took: 0:00:01 ======== Epoch 5 / 5 ======== Training... Average training loss: 0.13 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.4694706201553345 Validation took: 0:00:01 Training complete! Total training took 0:00:40 (h:mm:ss) . Evaluation . # Display floats with two decimal places. pd.set_option(&#39;precision&#39;, 2) # Create a DataFrame from our training statistics. df_stats = pd.DataFrame(data=training_stats) # Use the &#39;epoch&#39; as the row index. df_stats = df_stats.set_index(&#39;epoch&#39;) # Display the table. df_stats . Training Loss Valid. Loss Training Time Validation Time . epoch . 1 1.52 | 1.31 | 0:00:07 | 0:00:01 | . 2 1.34 | 1.18 | 0:00:07 | 0:00:01 | . 3 0.85 | 0.88 | 0:00:07 | 0:00:01 | . 4 0.32 | 0.46 | 0:00:07 | 0:00:01 | . 5 0.13 | 0.47 | 0:00:07 | 0:00:01 | . import matplotlib.pyplot as plt import seaborn as sns # Use plot styling from seaborn. sns.set(style=&#39;darkgrid&#39;) # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[&quot;figure.figsize&quot;] = (12,6) # Plot the learning curve. plt.plot(df_stats[&#39;Training Loss&#39;], &#39;b-o&#39;, label=&quot;Training&quot;) plt.plot(df_stats[&#39;Valid. Loss&#39;], &#39;g-o&#39;, label=&quot;Validation&quot;) # Label the plot. plt.title(&quot;Training &amp; Validation Loss&quot;) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.xticks([1, 2, 3, 4]) plt.show() . Look at results . So far, we can see that our model learns. Let&#39;s have a look how the predictions on our trainloader look like: . # Prediction on test set # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in train_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(&#39; DONE.&#39;) . DONE. DONE. DONE. DONE. DONE. . np.argmax(predictions[4], axis=1) . array([0, 4, 4, 0, 4, 3, 3, 3, 4, 3, 4]) . true_labels[4] . array([0, 4, 4, 0, 4, 3, 3, 3, 4, 3, 4]) . Well that looks right. However, this is the data we trained our model on. Way more interesting is the data our model hasn&#39;t trained on: . # Prediction on test set # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in validation_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(&#39; DONE.&#39;) . DONE. DONE. . np.argmax(predictions[0], axis=1) . array([4, 3, 2, 1, 4, 0, 4, 1, 0, 1, 3, 4, 3, 4, 1, 4]) . true_labels[0] . array([3, 3, 1, 1, 4, 0, 0, 1, 0, 1, 3, 4, 3, 4, 1, 4]) . Awesome! We have quite some variation in our prediction and most of the time they look pretty good! We save our model: . Save Model . torch.save(model.state_dict(), p/&#39;model/model_5epochs_lr1e-4.pt&#39;) . To further improve the model, I will get more data and then retrain it. However, I think so far the model does a pretty good job in replacing me when it comes to book recómmendations. . In the next part I will show you how to use Binder to make a small web application in which we can upload a picture of a page and then make a prediction how many stars I would probably give this book. . So stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/26/Use-pretrained-Bert-model-for-classification.html",
            "relUrl": "/2020/09/26/Use-pretrained-Bert-model-for-classification.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Teach Python how to read",
            "content": "Using PIL and pytesseract . In this blogpost I will show you how to make Python read text from an image and put it into a pandas dataframe. This is the first part of a three part project. I&#39;ve just taken these images with my phone and uploaded them into the storage of my Paperspace instance. To make this small project work we first have to make sure that tesseract is correctly installed. We can use bash commands from within Jupyter like that: . !apt-get update !apt-get install libleptonica-dev !apt-get install tesseract-ocr tesseract-ocr-dev !apt-get install libtesseract-dev . We then pip install pytesseract: . !pip install pytesseract . Before explaining everything in detail, I first run the code and then explain the parts. . Import packages . import numpy as np from pathlib import Path import pytesseract import re from PIL import Image, ImageFilter import pandas as pd . Define PosixPath to data . p = Path(&#39;../storage/data/book_text_images/&#39;) . img_paths = [x for x in p.iterdir()] . img_paths . [PosixPath(&#39;../storage/data/book_text_images/IMG_5076_derPate.jpg&#39;), PosixPath(&#39;../storage/data/book_text_images/IMG-5074_derPate.jpg&#39;), PosixPath(&#39;../storage/data/book_text_images/.ipynb_checkpoints&#39;), PosixPath(&#39;../storage/data/book_text_images/IMG_5078_derPate.jpg&#39;), PosixPath(&#39;../storage/data/book_text_images/IMG_5077_derPate.jpg&#39;)] . del img_paths[2] . img_paths[0].parts[4] . &#39;IMG_5076_derPate.jpg&#39; . Use regex to extract title of book . title_list = [re.match(&quot;^.* _(.*) ..*$&quot;,img_paths[i].parts[4]).group(1) for i in range(len(img_paths))] . Automatically read in images, transform them and get text . def proc_img(img_path): im1 = Image.open(img_path) im1 = im1.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None) im1 = im1.filter(ImageFilter.MedianFilter) return im1 . def get_text(img): return pytesseract.image_to_string(img, lang=&quot;deu&quot;) . def use_pattern(text): return pattern.sub(lambda m: rep[re.escape(m.group(0))], text) . rep = {&quot; n&quot;: &quot;&quot;, &quot;`&quot;: &quot;&quot;, &#39;%&#39;:&quot;&quot;, &#39;°&#39;: &#39;&#39;, &#39;&amp;&#39;:&#39;&#39;, &#39;‘&#39;:&#39;&#39;, &#39;€&#39;:&#39;e&#39;, &#39;®&#39;:&#39;&#39;, &#39; &#39;: &#39;&#39;, &#39;5&#39;:&#39;s&#39;, &#39;1&#39;:&#39;i&#39;, &#39;_&#39;:&#39;&#39;} # define desired replacements here # use these three lines to do the replacement rep = dict((re.escape(k), v) for k, v in rep.items()) #Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions pattern = re.compile(&quot;|&quot;.join(rep.keys())) . Use tesseract to make image into text . text_list = [use_pattern(get_text(proc_img(str(img_paths[i])))) for i in range(len(img_paths))] . text_list[0] . &#34;egenüberliegenden _Fenster stehen. Clemenza be t &#39; &#39;geraus und reichte Vlt0 em weißes Bündel. “ e Sich Weit„He, paisanl“ sagte Clemenza. —ich es wieder abh0ie. Schnell!“ Automatisch streckte Vit ’Vito sagte zu niemandem ein Wort, und seine verängstigteFrau wagte nicht den Mund aufzutun, aus Furcht, ihren Mannins Gefängnis zu bringen. Zwei Tage später tauchte Clemenzawieder auf und fragte Vito beiläuﬁg: „Hast du meine Sachennoch?“Vito nickte. Er war kein Freund vieler Worte. Clemenza kammit in die Wohnung hinauf und trank ein Glas Wein, währendVito das Bündel aus dem Schlafzimmerschrank holte.Clemenza musterte Vito mit einem abschätzenden Ausdruckauf seinem schweren, gutmütigen Gesicht. „Hast du es aufge-macht?“Vito schüttelte mit ausdruckleser Miene den Kopf. „Ichinteressiere mich nicht für Dinge, die mich nichts angehen.fDen ganzen Abend lang tranken sie Wem. Sie fanden ernan-der sympathisda. Clemenza war ein großer Geschichtenerzählerund Vito Corleone ein Mann, der einem Geschichtenerzähler gutzuhören konnte. So wurden sie Freunde.&#34; . Combine into Dataframe . d = {&#39;text&#39;:text_list,&#39;title&#39;:title_list} df = pd.DataFrame(d) df.head() . text title . 0 egenüberliegenden _Fenster stehen. Clemenza be... | derPate | . 1 ein bißchen unsicher, sei ganz natürlich. Sie ... | derPate | . 2 bewies, daß sie nicht zählten; daß er sie über... | derPate | . 3 und führte sie liebevoll in die Küche, diec&#39; T... | derPate | . As you see, the result is pretty good, although there are some mistakes. However, for the project at hand this suffices. Let me explain what exactly is happening here. . First of all, we define a PosixPath to our data folder (I prefer PathLib over os). Within this folder is still the checkpoint, which I d not want in my list, so I delete it. We then use a regex to extract the title of the book, the image is from, and put them in a list (use list comprehensions: they are fast and so incredebly useful!). . We then read in the images from our files and add some transformations. I tried a lot of transformations to improve the results I got from pytesseract: cropping, blurring, binarizing - it turned out there were only two things which really improves the results: rotating the image and filtering it. That&#39;s exactly what proc_img does. Let me show you what it does: . im1 = Image.open(str(img_paths[0])) im1.thumbnail((360,360)) im1.save(&#39;images/thumbnail_1.jpg&#39;) image1 = Image.open(&#39;images/thumbnail_1.jpg&#39;) image1 . im2 = im1.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None) im2.thumbnail((360,360)) im2.save(&#39;images/thumbnail_2.jpg&#39;) image2 = Image.open(&#39;images/thumbnail_2.jpg&#39;) image2 . im3 = im2.filter(ImageFilter.MedianFilter) im3.thumbnail((360,360)) im3.save(&#39;images/thumbnail_3.jpg&#39;) image3 = Image.open(&#39;images/thumbnail_3.jpg&#39;) image3 . The next step is to use pytesseract (in my case: german) to get the text from the image and then clean up a few common mistakes like &quot; n&quot; or confusing the letter &quot;s&quot; with &quot;5&quot;. That&#39;s the function get_text() in combination with use_pattern(). I then again use a list comprehension to put all the texts in one list, which in turn I use to build a dataframe with pandas. . And that&#39;s how you teach Python how to read. I guess there are children in pre-school with better reading abilities but, hey, it&#39;s good enough for our purpose of the project. . The next step will be to use this technique to build a large dataset from books I have read and rated, and use BERT to build a classifier which represents my taste of books. Finally, we want to build a small web application where you can upload an image from a page in a book, and the app will tell you what I will think of this book. Doesn&#39;t that sound exciting? . So stay tuned! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/22/Teach-Python-how-to-read.html",
            "relUrl": "/2020/09/22/Teach-Python-how-to-read.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "How to classify Lego figures",
            "content": "What better way to improve your significant other? . Build a Lego Classifier with fastai . !pip install kaggle . We want to save the dataset into the folder /notebooks/storage/data/Lego-Classification: . !kaggle datasets download -d ihelon/lego-minifigures-classification -p /notebooks/storage/data/Lego-Classification . Unzip data using Pythons pathlib library . from pathlib import Path p = Path(&#39;/notebooks/storage/data/Lego-Classification&#39;) filename = Path(&#39;/notebooks/storage/data/Lego-Classification/lego-minifigures-classification.zip&#39;) . Just as before, we can use bash commands from within Jupyter Notebook. So let&#39;s do that to unzip our data. -q is quiet mode, -d points to the direction where to unzip the data. Just see how well Pythons pathlib and bash work together! . !unzip -q {str(filename)} -d {str(p/&quot;train&quot;)} . Imports . from fastbook import * from fastai.vision.widgets import * import pandas as pd . Let&#39;s now use fastai&#39;s &quot;get_image_files()&quot; function to see how the unzipped data looks like in our destination path: . fns = get_image_files(p/&quot;train&quot;) fns . (#316) [Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/009.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/010.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/006.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/001.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/011.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/005.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/004.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/013.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/007.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/008.jpg&#39;)...] . Remember, we put the data into our directory &#39;/notebooks/storage/data/Lego-Classification&#39;. After having a quick look at our data it looks like the data is stored as follows: first the genre of our image (marvel/jurassic-world), then the classification of the figure (0001/0002 etc.). Within these folders we find many different pictures of that figure (001.jpg/002.jpg and so on). Let&#39;s confirm this by looking at the metadata. . df = pd.read_csv(f&#39;{p}/index.csv&#39;, index_col=0) df.tail(5) . path class_id train-valid . 311 marvel/0014/008.jpg | 28 | valid | . 312 marvel/0014/009.jpg | 28 | valid | . 313 marvel/0014/010.jpg | 28 | valid | . 314 marvel/0014/011.jpg | 28 | valid | . 315 marvel/0014/012.jpg | 28 | valid | . df_metadata = pd.read_csv(f&#39;{p}/metadata.csv&#39;, usecols=[&#39;class_id&#39;, &#39;lego_names&#39;, &#39;minifigure_name&#39;]) df_metadata.head() . class_id lego_names minifigure_name . 0 1 | [&#39;Spider Mech vs. Venom&#39;] | SPIDER-MAN | . 1 2 | [&#39;Spider Mech vs. Venom&#39;] | VENOM | . 2 3 | [&#39;Spider Mech vs. Venom&#39;] | AUNT MAY | . 3 4 | [&#39;Spider Mech vs. Venom&#39;] | GHOST SPIDER | . 4 5 | [&quot;Yoda&#39;s Hut&quot;] | YODA | . Indeed, that&#39;s how this dataset is structured. What we want is a data structure with which fastai&#39;s data block can easily work with. So what we need is something that gives us the filename, the label and a label which data is for training and which one is for validation. Luckily we can get exactly this by combining the meta-data: . datablock_df = pd.merge(df, df_metadata, left_on=&#39;class_id&#39;, right_on=&#39;class_id&#39;).loc[:,[&#39;path&#39;, &#39;class_id&#39;, &#39;minifigure_name&#39;, &#39;train-valid&#39;]] datablock_df[&#39;is_valid&#39;] = datablock_df[&#39;train-valid&#39;]==&#39;valid&#39; datablock_df.head() . path class_id minifigure_name train-valid is_valid . 0 marvel/0001/001.jpg | 1 | SPIDER-MAN | train | False | . 1 marvel/0001/002.jpg | 1 | SPIDER-MAN | valid | True | . 2 marvel/0001/003.jpg | 1 | SPIDER-MAN | train | False | . 3 marvel/0001/004.jpg | 1 | SPIDER-MAN | train | False | . 4 marvel/0001/005.jpg | 1 | SPIDER-MAN | train | False | . fastai gives us a brief overview of what to check before we can make optimal use of the datablock: . what are the types of our inputs and targets? Images and labels. where is the data? In a dataframe. how do we know if a sample is in the training or the validation set? A column of our dataframe. how do we get an image? By looking at the column path. how do we know the label of an image? By looking at the column minifigure_name. do we want to apply a function to a given sample? Yes, we need to resize everything to a given size. do we want to apply a function to a batch after it&#39;s created? Yes, we want data augmentation. . lego_block = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=ColSplitter(), get_x=lambda x:p/&quot;train&quot;/f&#39;{x[0]}&#39;, get_y=lambda x:x[2], item_tfms=Resize(224), batch_tfms=aug_transforms()) . Now our datablock is called lego_block. See how it perfectly matches together? . Let me briefly explain what the different steps within our lego_block are doing: first we tell the lego_block on what we want to split our data on (the default here is col=&#39;is_valid&#39;), then we simply put our path column (x[0]) and combine it with our path p and the folder &#39;train&#39; in which is is located in. get_y tells the lego_block where to find the labels in our dataset (x[2]), we then make all of our images the same size and apply transformation on them (checkout fastai for more information). . dls = lego_block.dataloaders(datablock_df) dls.show_batch() . Glorious! . fastai tries to make our life easier. This blog is intended to show you guys how to easily and quickly manage to get a great classifier with it. In the upcoming blogs I will try to better explain what is going on behind the scenes. But for now, let&#39;s enjoy how fast we can build our classifier with fastai! . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(20) . epoch train_loss valid_loss error_rate time . 0 | 5.044340 | 7.069784 | 0.980263 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 4.333549 | 5.407881 | 0.960526 | 00:05 | . 1 | 4.193551 | 4.404543 | 0.947368 | 00:04 | . 2 | 3.994802 | 3.605752 | 0.907895 | 00:04 | . 3 | 3.721432 | 2.941005 | 0.802632 | 00:04 | . 4 | 3.428093 | 2.336661 | 0.638158 | 00:05 | . 5 | 3.092875 | 1.841397 | 0.532895 | 00:04 | . 6 | 2.779624 | 1.470707 | 0.434211 | 00:05 | . 7 | 2.484196 | 1.200388 | 0.348684 | 00:05 | . 8 | 2.216630 | 1.011226 | 0.263158 | 00:05 | . 9 | 1.988927 | 0.901224 | 0.236842 | 00:04 | . 10 | 1.794780 | 0.819948 | 0.217105 | 00:05 | . 11 | 1.624516 | 0.756500 | 0.184211 | 00:04 | . 12 | 1.478024 | 0.716581 | 0.157895 | 00:05 | . 13 | 1.354157 | 0.688189 | 0.164474 | 00:04 | . 14 | 1.244102 | 0.673431 | 0.164474 | 00:05 | . 15 | 1.149104 | 0.662224 | 0.164474 | 00:04 | . 16 | 1.062147 | 0.652154 | 0.164474 | 00:04 | . 17 | 0.985224 | 0.654423 | 0.177632 | 00:04 | . 18 | 0.917764 | 0.653068 | 0.177632 | 00:05 | . 19 | 0.858115 | 0.652137 | 0.184211 | 00:04 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=2) . [(&#39;RON WEASLEY&#39;, &#39;HARRY POTTER&#39;, 2), (&#39;SPIDER-MAN&#39;, &#39;FIREFIGHTER&#39;, 2)] . Not to bad I would say. However, seeing an image of Ronald Weasley and predicting it to be Harry Potter - I&#39;m not sure how much this will impress your significant other. On the other hand, Captain America is correctly predicted 100%. . But we can still try to improve our model by unfreezing the weights, to make the model even better. Let&#39;s check this out: . learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 0.061034 | 0.536347 | 0.151316 | 00:05 | . 1 | 0.046480 | 0.631286 | 0.157895 | 00:04 | . 2 | 0.039729 | 0.572698 | 0.157895 | 00:05 | . Then we will unfreeze the parameters and learn at a slightly lower learning rate: . learn.unfreeze() . learn.fit_one_cycle(2, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.014817 | 0.483588 | 0.125000 | 00:05 | . 1 | 0.016834 | 0.425858 | 0.098684 | 00:04 | . Wow! Down to only 10% error rate. I think that&#39;s quite impressive! Let&#39;s see the confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . With these results I am sure you will impress your significant other! . In one of the next posts I will show you how to use Jupyter to easily set up a small Web App with the help of Binder. So stay tuned! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/16/Lego-Classification.html",
            "relUrl": "/2020/09/16/Lego-Classification.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "How to get the kaggle API started",
            "content": "Easy way to download datasets from kaggle from your terminal . In this blog post I would like to show you how to use kaggles api. And not only that, I will show you how to directly use this API from Jupyter. It&#39;s straightforward and shouldn&#39;t take longer than 5 minutes. Let&#39;s get started! . First of all, log in into your kaggle account and go to &quot;Your Account&quot;. Scroll down until you find the section API. Click on &quot;Create New API Token&quot;. . . Next, upload your downloaded token into a directory to which you have access to. In this blogpost, I use my Cloud Working Space on Paperspace (how to easily set up your own working space in Paperspace click here. . . Note however, that kaggle expects your token to be at a specific folder: ~/.kaggle/kaggle.json on Linux, OSX, and other UNIX-based operating systems, and at C: Users&lt;Windows-username.kaggle kaggle.json on Windows. So that&#39;s what we will do next. Just open your Terminal and move your token to the expected folder. . . And that&#39;s it, you can now use the kaggle API! If you (like me) use a remote computing instance, we don&#39;t want other user to possible use our token. We can prevent this by typing: . . Let&#39;s see how the kaggle API works. First of all, we need to pip install the package. . . If you are looking for a specific dataset, you can now use the kaggle API and simply type: . . If you want to download data from this API, you write: . . In that way you can easily access kaggle datasets and make that work even on cloud computing instances. . Stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/15/get-kaggle-api-started.html",
            "relUrl": "/2020/09/15/get-kaggle-api-started.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Setup Paperspace",
            "content": "Kostenlose (und bessere?) Alternative zu Google Colab . Die meisten Machine Learning bzw. Deep Learning Projekte will man nicht auf seinem eigenem PC/Laptop rechnen lassen. Dies hat viele gute Gründe. Zum einen hat nicht jeder Laptop eine geeignete Grafikkarte (GPU), auf der sich DeepLearning Modelle berechnen lassen. Außerdem ist das administrieren der verschiedenen Treiber und das Setup der GPU für Jupyter Notebooks nichts, was super einfach ist. Im Gegenteil. Zudem kostet eine gute Grafikkarte viel Geld. Alles Gründe, weswegen man sich lieber nach kostengünstigen Alternativen umsehen sollte, die einem den administrativen Aufwand abnehmen. . Eine solche Alternative ist Paperspace. Paperspace Gradient stellt ähnlich zu Google Colab eine kostenlose Alternative, wie sich in der Cloud Rechner mit GPUs benutzen lassen. Außerdem hat Paperspace Gradient den Vorteil, dass sich auch Daten wie Datensätze, Bilder, Modelle kostenlos speichern lassen, und nicht nach beenden der Instanz gelöscht werden (wie zum Beispiel in Google Colab). . In diesem Blog will ich zeigen, wie sich über Paperspace eine kostenlose Instanz anlegen lässt, die auch mit GPUs läuft, und wie sich die erstellten Notebooks einfach mit einem bestehenden github Repo verbinden lassen. . Zunächst muss man sich auf der Paperspace-Seite registrieren. Nach der einmaligen Registrierung öffnet sich Folgendes: . . Hier muss zunächst der Container gewählt werden. Das bedeutet nichts anderes, als dass man ein für ein bestimmtes Projekt benötigtes Grund-Setup schon für einen vorinstalliert bekommt - wie wunderbar! Man muss sich über keine Abhängigkeiten kümmern, ist sich sicher, dass die Packages auf dem aktuellsten Stand sind und falls benötigt kann man später immer noch selbst Packages hinzufügen. Was für ein Luxus! Wenn man z.B. ein Projekt mit PyTorch bauen will, so klickt man einfach auf den PyTorch Container. . . Als nächstes wählt man die Maschine, auf der das Projekt laufen soll. Das Beste hierbei: es gibt Maschinen, die von Paperspace komplett umsonst zur Verfügung gestellt werden, sogar mit GPU! Diese sind die letzten 3 Auswahlmöglichkeiten. Und wenn man mehr Power braucht, so kann man auch im Nachhinein noch aufrüsten. . Des weiteren kann man ganz einfach ein bestehendes eigenes git-Repo mit der Instanz verbinden. Dafür einfach den git-Link zum Repo einfügen. . . Zuletzt muss noch eine gültige Kreditkarte hinterlegt werden (wie schon beschrieben, die Kreditkarte wird nur belastet, wenn eine der nicht kotenlosen Alternativen gewählt wird). Danach kann auf &quot;Create new Instance&quot; geklickt werden und schon geht es los! . . Jetzt kann auf &quot;Open&quot; geklickt werden und schon startet sich ein Jupyter Notebook. Das Ganze sollte dann in etwa so aussehen (das README.md stammt schon aus dem verlinkten github-Repo): . . Wie erwähnt ist mein github Repo schon mit der Instanz verbunden. So sieht bislang mein neu angelegtes Repo in github aus: . . Dann lasst uns ein neues Jupyter Notebook anlegen. Einfach auf &quot;New&quot; -&gt; &quot;Python 3&quot; klicken, und es öffnet sich ein Notebook: . . Als nächstes will ich zeigen, wie sich mit Hilfe des Terminals ganz einfach neue packages installieren lassen (über pip), und wie sich das bestehende github Repo aus dem Terminal ansteuern lässt. Dafür muss man bei Jupyter einfach auf &quot;New&quot; und dann -&gt; &quot;Terminal&quot; klicken. . . Mit dem Befehl installieren wir aus fastai das fastbook. Dies funktioniert über pip install. . Jetzt will ich zeigen, wie sich das Terminal ganz einfach mit git verbinden lässt. Über git clone können wir öffentliche git-Repos direkt auf unsere Instanz klonen/kopieren: . . Dies finden wir auch direkt in Jupyter wieder: . . Jetzt wollen wir das Ganze in unser mit der Instanz verbundenem github Repo laden. Dies funktioniert über git add, git commit und git push im Terminal. . . Anschließend muss noch der User Name zum Repo angegeben werden und das dazugehörige Passwort. Das war&#39;s schon. Wie großartig ist das? So sieht das geupdatete github Repo aus: . . Als weitere großartige Möglichkeit bietet Paperspace in den extra dedizierten Ordnern &quot;datasets&quot; und &quot;storage&quot; an, eigene Dateien umsonst in Paperspace zu speichern. Das bedeutet, dass sich bei Neustarten der Instanzen oder neu anlegen der Instanzen diese Dateien immer da sind! Diese Funktionalität bietet Google Colab zum Beispiel nicht an. . Ich hoffe dieser Blog war hilfreich und ihr seid wenigstens halb so begeistert wie ich es bin. . Bleibt dran für die nächsten Posts! . Euer Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/12/Setup-Paperspace.html",
            "relUrl": "/2020/09/12/Setup-Paperspace.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Was ist Gradient Descent und wie funktioniert es?",
            "content": "Eine Coding Anleitung . (dieser Beitrag basiert lose auf dem von fastai zur Verfügung gestellten fastbook) . Dieser Blog Post ist nicht dafür gedacht aufzuzeigen, wie die mathematische Berechnung hinter Gradient Descent funktioniert. Denn zum Glück gibt es genau dafür Computer, die das Ganze wahrscheinlich millionenfach schneller berechnen können als wir. Ich will eine intuitive Erklärung für Gradient Descent liefern, wofür wir Gradient Descent überhaupt brauchen und wie wir mit simplen Python Code unseren eigenen auf Gradient Descent beruhenden Algorithmus bauen können. . Ich erinnere mich, dass ich zu Schulzeiten (und mehr als Abi-Mathe wird für diesen Post wahrhaftig nicht gebraucht) die Ableitungsregeln gelernt und auf Funktionen angewandt habe, aber wirklich Sinn und Zweck habe ich in dem Ganzen nie gesehen. Im besten Fall hat sich die Lehrerin einen an den Haaren herbeigezogenen Fall ausgedacht, um nicht einfach stumpf die Funktion zu liefern, die abgeleitet werden soll. Ich hoffe nach diesem Blog wird klarer, weswegen Ableitungen bzw. die dazugehörigen Gradienten doch eine ziemlich coole und nützliche Sache sind. . Zunächst einmal möchte ich zeigen, was Gradient Descent kann: . . Ein Algorithmus, der erkennen kann, was für ein Bär auf einem gegebenem Bild zu sehen ist? Was zur Hölle hat das mit Gradient Descent zu tun? Wenn du es bis zum Ende des Posts aushältst, wirst du das hoffentlich verstehen. . Gradient Descent hilft Computern dabei zu lernen. Das Ganze sollten wir uns vielleicht in einem Schaubild einmal näher anschauen: . . Die Idee ist dabei wie folgt: Wir haben ein Modell, welches anhand von Bildern erkennen soll, welche Art von Bär hier zu sehen ist. Nachdem wir genügend Bilder von verschiedenen Bären gesammelt haben, fangen wir an unser Modell zu fragen, was es glaubt für eine Art von Bären zu sehen (predict). Wir können uns nun Metriken überlegen, an denen wir erkennen können, wie gut diese Vorhersage unseres Modells ist, zum Beispiel wie häufig das Modell die richtige Bärenart vorhergesagt hat und wie häufig es falsch lag. Wir können daraus einen sogenannten Loss berechnen, was nichts anderes ist als das, was die Lehrer uns immer als Funktion beschrieben haben. Wir haben also eine Funktion die uns angibt, wie gut/schlecht unser Modell Bärenarten vorhersagen kann. Diese Funktion kann zum Beispiel eine simple quadratische Funktion sein, sie kann aber theoretisch jede nur erdenkliche Form annehmen. . Wir wollen unseren Loss minimieren, sprich unsere Funktion soll so gut wie es nur kann die Funktion lernen, wie es möglichst gut Bärenarten voneinander unterscheiden kann. Lass uns eine Sekunde darüber nachdenken. Wir benutzen eine Loss-Funktion, damit wir unserem Modell Feedback geben, wie gut/schlecht es die bisherige Aufgabe gelöst hat. Mithilfe dieser Loss-Funktion lernt unser Modell, die verschiedenen Bärenarten besser zu unterscheiden. Doch wie &quot;lernt&quot; das Modell? Hier kommt Gradient Descent ins Spiel. . Der Gradient, also die Ableitung, gibt uns an, um wie viel die Funktion größer wird, wenn wir (optisch gesprochen) einen kleinen Schritt nach rechts bzw. einen kleinen Schritt nach links gehen von dem Punkt, an dem wir uns gerade befinden. Dies ist die Steigung von dem Punkt, an welchem wir uns gerade befinden. Doch was bringt uns das? . Ich hoffe, du hast einen Moment darüber nachgedacht. Was wir wollen, ist möglichst gut die Bärenarten voneinander zu unterscheiden. Dies steuern wir über unsere Loss-Funktion. Und je geringer unsere Loss-Funktion, desto besser sind wir im Vorhersagen, was für eine Bärenart wir hier gerade haben. Durch den Gradienten wissen wir, in welche Richtung wir uns bewegen müssen, um unsere Loss-Funktion kleiner zu machen. . Noch mag das Ganze recht abstrakt klingen, schon in Kürze folgt hier das Beispiel in Python. Ich will das Ganze aber noch einmal zusammenfassen. Wir wollen etwas optimieren, zum Beispiel möglichst genau die Bärenart vorhersagen. Um diese Funktion zu optimieren, brauchen wir die Loss-Funktion, die uns angibt, wie gut/schlecht unser Modell/unsere Zielfunktion performt. Diese Funktionen können jegliche erdenkliche Formen annehmen. Dem Gradienten ist dies jedoch egal. Der Gradient kann uns zu jedem Ort, an welchem wir uns in der Funktion befinden sagen, was mit unser Loss-Funktion passiert, wenn wir uns ein kleines Stück in eine beliebeige Richtung bewegen. Dies nutzen wir als Feedback, um die Parameter der Zielfunktion so anzupassen, dass die Loss-Funktion kleiner wird, wir also besser die Bärenarten voneinander unterscheiden können. . Dies sind die Schritte, die in dem Schaubild erklärt sind: wir initialisieren die Werte unserer Zielfunktion (anfangs zufällig, weil wir nicht wissen, wie die richtige Funktion aussieht), wir lassen unser Modell Vorhersagen treffen, berechnen daraufhin den Loss und die dazugehörigen Gradienten, um dann durch die Gradienten die Parameter des Modells anzupassen. Diesen Prozess wiederholen wir solange, bis wir mit dem Endergebnis zufrieden sind .Dies sollte auf einem sogenannten Validierungs-Set festgelegt werden, also auf Bildern von Bären, die unser Modell im Trainingsloop nicht sieht. Vielleicht wäre es hier angebracht einmal darüber nachzudenken, warum wir nicht einfach den Trainingsloop solange wiederholen, bis wir alle Bärenarten durch Gradient Descent korrekt vorhersagen können (Stichwort: Overfitting). . Genug geredet, jetzt wollen wir das Ganze auch in Code sehen! . Code Beispiel . Wir wollen mit Hilfe des oben beschriebenen Prozesses eine Funktion finden, die möglichst genau den Verlauf folgender Funktion beschrieben kann: . import torch import matplotlib.pyplot as plt . x = torch.arange(-5,20).float(); x y = 0.75*(x-4)**2 + 0.5*x + 1 plt.scatter(x,y); . Was wir wollen ist die Parameter dieser Funktion zu schätzen. Vom ansehen der Daten können wir bereits auf die funktionale Form schließen - ein Polynom 3ten Grades. Was wir an diesem Beispiel schon erkennen können ist, dass die angenommene Zielfunktion eine große Rolle spielt. Hätten wir von den Daten auf eine quadratische Funktion geschlossen, würden wir die &quot;wahre&quot; Form der Funktion nie richtig bestimmen können. Deep Learning überkommt dieses &quot;Problem&quot;, indem es jede nur erdenkliche Funktion annehmen kann (dazu mehr in einem späteren Post). . def f(x, params): a,b,c,d = params return a*(x-b)**2 + (c*x) + d . Jetzt benötigen wir noch unsere Loss-Funktion (und ich hoffe hier wird klar, wie Loss-Funktion und Zielfunktion miteinander &quot;kommunizieren&quot;): . def mse(preds, targets): return ((preds-targets)**2).mean() . params = torch.randn(4).requires_grad_() params . tensor([ 1.1514, 0.2004, -0.8726, 0.5281], requires_grad=True) . Wir starten unsere Vorhersagen: . preds = f(x, params) plt.scatter(x,preds.detach().numpy()) . &lt;matplotlib.collections.PathCollection at 0x7fd49eb1e580&gt; . Wie gut sehen unsere Vorhersagen aus? . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(x, y) ax.scatter(x, preds.detach().numpy(), color=&#39;red&#39;) ax.set_ylim(-10,100) ax.set_xlim(-5,20) . show_preds(preds) . Wow! Ziemlich miserabel. Wie miserabel zeigt uns unser loss. . loss = mse(preds, y) loss . tensor(8606.2266, grad_fn=&lt;MeanBackward0&gt;) . Auf geht&#39;s Gradient! Zeig uns, wie wir unsere Parameter updaten müssen, damit der Loss kleiner wird. . loss.backward() params.grad . tensor([26820.2754, -4137.0151, 1819.5073, 114.5496]) . Dann lass uns unsere Paramter anpassen (wir multiplizieren den Gradienten mit der sogenannten Learning Rate, dazu in einem weiteren Blog Posts mehr). . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None params . tensor([ 0.8832, 0.2418, -0.8908, 0.5269], requires_grad=True) . Ist unser Loss geringer geworden? . preds = f(x,params) mse(preds, y) . tensor(2857.6997, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . Zum Glück ja. . Wir wollen die steps wiederholen, sodass wir langsam und mithilfe von Gradient Descent unsere Zielfunktion finden. . def apply_step(params, prn=True): preds = f(x, params) loss = mse(preds, y) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds lr=1e-5 for i in range(20): apply_step(params) . 204.3289794921875 201.9536590576172 201.0904083251953 200.72898864746094 200.5342254638672 200.39480590820312 200.27386474609375 200.1591033935547 200.0463104248047 199.9343719482422 199.82254028320312 199.71095275878906 199.59934997558594 199.4879150390625 199.3763885498047 199.26504516601562 199.1535186767578 199.04220581054688 198.93089294433594 198.8196258544922 . _,axs = plt.subplots(1,6,figsize=(24,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Wie man sieht, passt sich die rote Kurve, also unsere Vorhersagen, immer mehr der wahren Kurve an. Alles aufgrund von Gradient Descent! Und genau diese Technik und diese Schritte, die hier in diesem Blog Post aufgezeigt wurden, sind auch die Schritte, die dabei helfen, Neuronale Netze zu trainieren. Die dann wiederum Bären auseinander halten können. . Ich hoffe, durch diesen Post ist die Idee hinter Gradient Descent ein wenig greifbarer geworden und der Sinn und Zweck von Ableitungen könnte von einer anderen Seite vielleicht ein wenig verständlicher betrachtet werden. . Bleibt dran für die nächsten Posts! . Euer Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/01/Gradient-Descent.html",
            "relUrl": "/2020/09/01/Gradient-Descent.html",
            "date": " • Sep 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Ich bin Lasse, ein Data Science Enthusiast! .",
          "url": "https://lschmiddey.github.io/fastpages_/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lschmiddey.github.io/fastpages_/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}