{
  
    
        "post0": {
            "title": "How to use fastai tabular with custom metric",
            "content": "How to easily train a tabular model with fastai . First, we have to load our data (I used the kaggle API to do this, check out my previous blogpost) . from pathlib import Path p = Path(&#39;/notebooks/storage/data/Titanic&#39;) filename = Path(&#39;/notebooks/storage/data/Titanic/titanic.zip&#39;) . !unzip -q {str(filename)} -d {str(p/&quot;train&quot;)} . Get imports . import pandas as pd from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.metrics import confusion_matrix . df = pd.read_csv(f&#39;{p}/train/train.csv&#39;, index_col=0) df.tail(5) . Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . PassengerId . 887 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.00 | NaN | S | . 888 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.00 | B42 | S | . 889 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.45 | NaN | S | . 890 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.00 | C148 | C | . 891 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.75 | NaN | Q | . df.shape . (891, 11) . Let&#39;s check out the data type for each column. . df.dtypes . Survived int64 Pclass int64 Name object Sex object Age float64 SibSp int64 Parch int64 Ticket object Fare float64 Cabin object Embarked object dtype: object . We see that &quot;Pclass&quot;, &quot;Sex&quot;, &quot;Cabin&quot; and &quot;Embarked&quot; are objects. When we think about these variables, they clearly are categorical variables. So let&#39;s change that. Also, this blogpost is not about how to train the best possible model, so we do not want any fancy feature engineering. I therefore will get rid of the variable &quot;Name&quot;. . df[&#39;Pclass&#39;] = df[&#39;Pclass&#39;].astype(&#39;category&#39;) df[&#39;Sex&#39;] = df[&#39;Sex&#39;].astype(&#39;category&#39;) df[&#39;Cabin&#39;] = df[&#39;Cabin&#39;].astype(&#39;category&#39;) df[&#39;Embarked&#39;] = df[&#39;Embarked&#39;].astype(&#39;category&#39;) . df = df.drop(columns=[&#39;Name&#39;]) . I want to build a validation set, based on the index. The easiest way is to create an array with the length of our data, randomly select a percentage of indeces we want in our training data, and use the leftover indeces for our validation set. . # Define percentage of train data, define length of training indeces and validation indeces p = 0.9 len_df = len(df) len_idx_tain = round(len_df*p) len_idx_val = len_df-len_idx_tain . # build array of indeces for training and validation idx_arr = range(0,len_df) train_idx = np.random.choice(range(0,len_df), len_idx_tain, replace=False) val_idx = [i for i in idx_arr if i not in train_idx] val_idx = np.asarray(val_idx) . Let&#39;s put these arrays into a split variable, which fastai knows how to make use of to split the data into training data and validation data. . splits = (list(train_idx),list(val_idx)) . Then we make use of the &quot;cont_cat_split&quot; function to easily get a list of column names for each categorical and continuous variable. . cont_nn, cat_nn = cont_cat_split(df, dep_var=&#39;Survived&#39;) . cont_nn, cat_nn . ([&#39;Age&#39;, &#39;Fare&#39;], [&#39;Pclass&#39;, &#39;Sex&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Cabin&#39;, &#39;Embarked&#39;]) . That looks great! How about our missing values? . df.isna().sum() . Survived 0 Pclass 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . Looks like there are quite a few missing values for age and cabin. But we know a way to make use of that! Even &quot;no-information&quot; can be an information. . procs_nn = [Categorify, FillMissing, Normalize] . Fastai provides us with many super useful out-of-the-box functions. Three of them you can see here: Categorify will take all of the variables with type &quot;category&quot; will replace the category with a numerical number and write the mapping into a dictionary. FillMissing, well, does exactly this. It also provides us with a extra boolean variable, indicating whether the row has a missing value for a specific variable (e.g. age). Normalize then normalizes our continuous data and saves the standardzier. . df[&#39;Survived&#39;] = df[&#39;Survived&#39;].astype(np.float32) . This one is important: fastai needs the dependend variable to be of type float32, even though we have a binary classification! We need this, because the loss function will be mse-loss. This leads to possibly odd behavior: the numbers can get below 0 and even below -1 or above 2! If we use a provided metric like accuracy, this leads to odd results. So we will write our own custom metric! But first, let&#39;s use the TabularPandas class and put that in a dataloader (important: do not let the batchsize become bigger than your data, this will result in a error!). . to_nn = TabularPandas(df, procs_nn, cat_nn, cont_nn, splits=splits, y_names=&#39;Survived&#39;) # length of data is 891, so let the batchsize be smaller than that! dls = to_nn.dataloaders(256) . Train Model . Now is the time to train our model. But wait! We first have to define a custom metric, because the accuracy provided by fastai will lead to odd results. I can show you what I mean: . learn = tabular_learner(dls, layers=[64, 8], n_out=1, metrics=[accuracy]) . learn.fit_one_cycle(4,1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.562715 | 0.504298 | 0.617977 | 00:00 | . 1 | 0.498489 | 0.477128 | 0.617977 | 00:00 | . 2 | 0.436740 | 0.448243 | 0.617977 | 00:00 | . 3 | 0.391733 | 0.423635 | 0.617977 | 00:00 | . What is going on here? Is the accuracy not improving? The answer is no. Tabular learner in combination with accuracy does not give us the result we want. Why is that? First, our predictions will not be 0 or 1, but floating point values. That&#39;s good for our loss, so our model can learn from its gradient (checkout my blogpost about gradients if you do not know what that means). But this leads to a malfunctioning of the accuracy metric, which fastai uses from sklearn. What we need to do is to define our own custom metric. Let&#39;s do that! . import sklearn.metrics as skm def _accumulate(self, learn): #pred = learn.pred.argmax(dim=self.dim_argmax) if self.dim_argmax else learn.pred m = nn.Sigmoid() pred = learn.pred pred = torch.round(m(pred)) targ = learn.y pred,targ = to_detach(pred),to_detach(targ) self.preds.append(pred) self.targs.append(targ) AccumMetric.accumulate = _accumulate def BinAccu(): return skm_to_fastai(skm.accuracy_score) . So what&#39;s going on here? Next to Callbacks, fastai provides a clever way how to customize metrics. To be honest, it took me some time to figure it out, but here&#39;s how it works. Each learner has a .pred and a .y, which means this is how you can get its predictions and its targets. As I already mentioned, preds are floating point values, but we want them to be between 0 and 1 - luckily, this is exactly what the sigmoid function is doing. Then we want the values to be either 0 or 1, so let&#39;s just round these values (threshold here is .5, but you can fiddle around with that). We then append these &quot;new&quot; preds and targets. . &quot;In order to provide a more flexible foundation to support metrics like this fastai provides a Metric abstract class which defines three methods: reset, accumulate, and value (which is a property). Reset is called at the start of training, accumulate is called after each batch, and then finally value is called to calculate the final check.&quot; . So with this function we directly sit on top of accumulate. The function skm_to_fastai let&#39;s you use sklearn metrics (in this case: accuracy_score) and uses the pred and targ we provided in our tiny function. Important: we have to instanciate the instance first! . binaccu = BinAccu() learn = tabular_learner(dls, n_out=1, metrics=[binaccu]) . Ok, we&#39;re good to go. Let&#39;s use fastai&#39;s awesome lr_find(). . learn.lr_find() . SuggestedLRs(lr_min=0.03630780577659607, lr_steep=0.0010000000474974513) . Looks like we should set our learning rate to about 1e-3. . learn.fit_one_cycle(10,1e-3) . epoch train_loss valid_loss accuracy_score time . 0 | 0.484679 | 0.372455 | 0.325843 | 00:00 | . 1 | 0.425642 | 0.374143 | 0.595506 | 00:00 | . 2 | 0.343880 | 0.367273 | 0.494382 | 00:00 | . 3 | 0.271155 | 0.349920 | 0.393258 | 00:00 | . 4 | 0.220030 | 0.334517 | 0.382022 | 00:00 | . 5 | 0.184440 | 0.325883 | 0.382022 | 00:00 | . 6 | 0.157990 | 0.319556 | 0.382022 | 00:00 | . 7 | 0.137255 | 0.313525 | 0.382022 | 00:00 | . 8 | 0.120796 | 0.309076 | 0.382022 | 00:00 | . 9 | 0.107582 | 0.306098 | 0.382022 | 00:00 | . Our accuracy is improving until epoch 2. So we should restart our training. But first, let&#39;s have a look, whether our accuracy score is doing what we expect: . preds, targs = learn.get_preds() m = nn.Sigmoid() confusion_matrix(torch.round(m(preds.view(-1))).numpy(), targs.view(-1).numpy()) . array([[ 0, 0], [55, 34]]) . 1-abs(torch.round(m(preds.view(-1))) - targs.view(-1)).sum()/len((torch.round(m(preds.view(-1))) - targs.view(-1))) . tensor(0.3820) . Awesome. That&#39;s exactly what we wanted (not the result though, which is pretty bad ;) ) . Let&#39;s re-run the trainig. . binaccu = BinAccu() learn = tabular_learner(dls, layers=[128,128], n_out=1, metrics=[binaccu]) . learn.fit_one_cycle(5,1e-5) . epoch train_loss valid_loss accuracy_score time . 0 | 0.616267 | 0.422966 | 0.617978 | 00:00 | . 1 | 0.610159 | 0.418977 | 0.617978 | 00:00 | . 2 | 0.607095 | 0.416566 | 0.617978 | 00:00 | . 3 | 0.604457 | 0.414736 | 0.629213 | 00:00 | . 4 | 0.604596 | 0.413118 | 0.640449 | 00:00 | . preds, targs = learn.get_preds() m = nn.Sigmoid() confusion_matrix(torch.round(m(preds.view(-1))).numpy(), targs.view(-1).numpy()) . array([[54, 31], [ 1, 3]]) . To be honest, the results aren&#39;t quite that good. With a RandomForest you can easily get up to 85% accuracy. However, in this blogpost I wanted to show you how to leverage fastai&#39;s tabular_learner and add a custom metric to it. . I hope you stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/10/01/Tabular-Data-with-custom-metric.html",
            "relUrl": "/2020/10/01/Tabular-Data-with-custom-metric.html",
            "date": " • Oct 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Build a web app with binder",
            "content": "Let&#39;s deploy the model! . This is the last part of my small project to get an app running, which will take an image as input, extract the text and then run a deep learning model on it to give the book a rating based on my book taste. In part 3 we put together the parts we need to create our web app with binder and voila. . The final project you&#39;ll find in my git repo. This blogpost should guide you through the git repo, what to do to build your web app from a jupyter notebook. Ok, so let&#39;s get started! . . Alright, what have we got here? First of all, the jupyter notebook from part 3. So nothing new here. We have a LICENSE and a README file, which aren&#39;t that important. What&#39;s next is important, the three files &quot;app.yml&quot;, &quot;apt.txt&quot; and &quot;requirements.txt&quot;. Let&#39;s start with &quot;requirements.txt&quot;. . . In the &quot;requirements.txt&quot; are all the packages we need for our project. Binder will create a docker container for us, so it needs to know what packages are required. Next, let&#39;s have a look at &quot;apt.txt&quot;. . . This one is moderatly more difficult. Especially if you do haven&#39;t worked with docker before. It looks like the requirement file, however what this does is it apt installs these packages. This is required, because pytesseract works different from other packages. Without this file our jupyter notebook won&#39;t be able to connect to pytesseract. I had a lot of head scratching and googling to do before I found out that tesseract needs to be installed that way to find the path where its libraries are. . Next, let&#39;s look at app.yml. . . This file is needed to that we can make use of voila. Voila takes all the non html Output, runs it in the background and doesn&#39;t show it to the user of the app. . Ok, we&#39;re finally able to start our web app on Binder. . . So we simply specify the path to our git repository and type &quot;/voila/render/App-with-Voila-mybinder.ipynb&quot; into the URL to open. Then instead of &quot;File&quot; we use &quot;URL&quot;. . Ok, let Binder do its work and set up a docker container for us, on which our app is running. When Binder creates the docker container for the project for the first time, this will take a while (5 min or so). . Let&#39;s have a look at the running app. . . Let&#39;s uploade an image and see what my algorithm will say how much I will like this book: . . Awesome! That looks great (btw. the book was the sea wolf by Jack London and I loved this book). Now everyone can use this little web app and see whether I would recommend this book or not. Simply click on the &quot;launch binder&quot; in my git repo and start the app. I hope you enjoyed this little project as much as I did and stay tuned for more. . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/28/Build-binder-app-Part4.html",
            "relUrl": "/2020/09/28/Build-binder-app-Part4.html",
            "date": " • Sep 28, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Lasse's book recommender",
            "content": "How would Lasse rate this book? . This is the third part of my little project to build a rating system on text which we extract from images and which in turn leads to a rating on how much I will like this book. In this notebook I want to show you how to make use of ipywidgets to make a notebook which we can use as a web appplication. Furthermore, I will show you how to download the trained model from part 2 from my private GoogleDrive. So let&#39;s get started! . !pip install googledrivedownloader !pip install transformers . Collecting googledrivedownloader Downloading googledrivedownloader-0.4-py2.py3-none-any.whl (3.9 kB) Installing collected packages: googledrivedownloader Successfully installed googledrivedownloader-0.4 Collecting transformers Downloading transformers-3.3.0-py3-none-any.whl (1.1 MB) |████████████████████████████████| 1.1 MB 14.9 MB/s eta 0:00:01 |██▉ | 92 kB 15.2 MB/s eta 0:00:01 |███████████████████████████████ | 1.0 MB 14.9 MB/s eta 0:00:01 Requirement already satisfied: tqdm&gt;=4.27 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (4.48.2) Requirement already satisfied: requests in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (2.24.0) Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (0.1.86) Requirement already satisfied: numpy in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (1.19.1) Collecting filelock Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB) Collecting regex!=2019.12.17 Downloading regex-2020.9.27-cp38-cp38-manylinux2010_x86_64.whl (675 kB) |████████████████████████████████| 675 kB 7.2 MB/s eta 0:00:01 Collecting sacremoses Downloading sacremoses-0.0.43.tar.gz (883 kB) |████████████████████████████████| 883 kB 25.0 MB/s eta 0:00:01 Requirement already satisfied: packaging in /opt/conda/envs/fastai/lib/python3.8/site-packages (from transformers) (20.4) Collecting tokenizers==0.8.1.rc2 Downloading tokenizers-0.8.1rc2-cp38-cp38-manylinux1_x86_64.whl (3.0 MB) |████████████████████████████████| 3.0 MB 23.4 MB/s eta 0:00:01 Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (2020.6.20) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from requests-&gt;transformers) (1.25.10) Requirement already satisfied: six in /opt/conda/envs/fastai/lib/python3.8/site-packages (from sacremoses-&gt;transformers) (1.15.0) Collecting click Downloading click-7.1.2-py2.py3-none-any.whl (82 kB) |████████████████████████████████| 82 kB 1.3 MB/s eta 0:00:01 Requirement already satisfied: joblib in /opt/conda/envs/fastai/lib/python3.8/site-packages (from sacremoses-&gt;transformers) (0.16.0) Requirement already satisfied: pyparsing&gt;=2.0.2 in /opt/conda/envs/fastai/lib/python3.8/site-packages (from packaging-&gt;transformers) (2.4.7) Building wheels for collected packages: sacremoses Building wheel for sacremoses (setup.py) ... done Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=5a3671aa51ef5e5501f57c816f31eea01e646340384391c47489f75a0c3cb57c Stored in directory: /root/.cache/pip/wheels/7b/78/f4/27d43a65043e1b75dbddaa421b573eddc67e712be4b1c80677 Successfully built sacremoses Installing collected packages: filelock, regex, click, sacremoses, tokenizers, transformers Successfully installed click-7.1.2 filelock-3.0.12 regex-2020.9.27 sacremoses-0.0.43 tokenizers-0.8.1rc2 transformers-3.3.0 . from fastai.vision.all import * from fastai.vision.widgets import * from fastai.vision.widgets import * from PIL import Image, ImageFilter import pytesseract import re from transformers import BertTokenizer, BertForSequenceClassification from pathlib import Path from torch.utils.data import TensorDataset, DataLoader . Lots of models especially in the deep learning context can get quite large. I wasn&#39;t able to upload my model into git, so I thought of a way to get around that. I uploaded my trained model from part 2 into my GoogleDrive and then use the google_drive_downloader to download my model into my notebook. . from google_drive_downloader import GoogleDriveDownloader as gdd gdd.download_file_from_google_drive(file_id=&#39;1kk_SvwpwZeuLnZirW5vbrd8FEnm7yJRt&#39;, dest_path=&#39;./export.pkl&#39;, unzip=True) . Downloading 1kk_SvwpwZeuLnZirW5vbrd8FEnm7yJRt into ./export.pkl... Done. Unzipping...Done. . import warnings warnings.filterwarnings(&quot;ignore&quot;) . Next, we use all the steps you already know from part 2: rotate the image and filter it, use pytesseract to extract the text from the image, tokenize the text and put it in a dataloader and download the pre-trained model from the awesome huggingface library. . def proc_img(input_img): img = input_img.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None) img = img.filter(ImageFilter.MedianFilter) return img . def get_text(img): return pytesseract.image_to_string(img, lang=&quot;deu&quot;) . def use_pattern(text): return pattern.sub(lambda m: rep[re.escape(m.group(0))], text) . rep = {&quot; n&quot;: &quot;&quot;, &quot;`&quot;: &quot;&quot;, &#39;%&#39;:&quot;&quot;, &#39;°&#39;: &#39;&#39;, &#39;&amp;&#39;:&#39;&#39;, &#39;‘&#39;:&#39;&#39;, &#39;€&#39;:&#39;e&#39;, &#39;®&#39;:&#39;&#39;, &#39; &#39;: &#39;&#39;, &#39;5&#39;:&#39;s&#39;, &#39;1&#39;:&#39;i&#39;, &#39;_&#39;:&#39;&#39;, &#39;-&#39;:&#39;&#39;} # define desired replacements here # use these three lines to do the replacement rep = dict((re.escape(k), v) for k, v in rep.items()) #Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions pattern = re.compile(&quot;|&quot;.join(rep.keys())) . # Tokenize all of the sentences and map the tokens to thier word IDs. def tokenize_text(sent): input_ids = [] attention_masks = [] encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; truncation=True, max_length = 256, # Pad &amp; truncate all sentences. pad_to_max_length = True, #padding=&#39;longest&#39;, return_attention_mask = True, # Construct attn. masks. return_tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding). attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.cat(input_ids, dim=0) attention_masks = torch.cat(attention_masks, dim=0) return input_ids, attention_masks . def create_dataloader(text): input_ids, attention_masks = tokenize_text(text) dataset = TensorDataset(input_ids, attention_masks) batch_size = 1 app_dataloader = DataLoader( dataset, # The validation samples. batch_size = batch_size # Evaluate with this batch size. ) return app_dataloader . def predict(dataloader): # Prediction on test set device = torch.device(&#39;cpu&#39;) # Put model in evaluation mode model.eval() # Tracking variables predictions = [] # Predict for batch in dataloader: # Add batch to CPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask = batch # Telling the model not to compute or store gradients, saving memory and # speeding up prediction with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() # Store predictions and true labels predictions.append(logits) return np.argmax(predictions) . PRE_TRAINED_MODEL_NAME = &#39;bert-base-german-cased&#39; # Load the BERT tokenizer tokenizer = torch.hub.load(&#39;huggingface/pytorch-transformers&#39;, &#39;tokenizer&#39;, PRE_TRAINED_MODEL_NAME) # Download vocabulary from S3 and cache. n_classes=5 model = BertForSequenceClassification.from_pretrained( &quot;bert-base-german-cased&quot;, # Use the 12-layer BERT model, with an uncased vocab. num_labels = n_classes, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) . Downloading: &#34;https://github.com/huggingface/pytorch-transformers/archive/master.zip&#34; to /root/.cache/torch/hub/master.zip . . p = Path.cwd() . Even though we trained the model on GPU, that&#39;s not what we want for production. So I load my model onto CPU. . device = torch.device(&#39;cpu&#39;) model.load_state_dict(torch.load(p/&#39;export.pkl&#39;, map_location=device)) . btn_upload = widgets.FileUpload() out_pl = widgets.Output() rating_widget = widgets.Label() btn_run = widgets.Button(description=&#39;Lasses Empfehlung:&#39;) . def on_click_text(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(proc_img(img).to_thumb(256,256)) text = use_pattern(get_text(proc_img(img))) star_rating = predict(create_dataloader(text)) rating_widget.value = f&#39;Lasse würde diesem Buch {star_rating+1} Stern(e) von 5 Sternen geben!&#39; . btn_run.on_click(on_click_text) . VBox([widgets.Label(&#39;Upload Bild von Buchseite&#39;), btn_upload, btn_run, out_pl, rating_widget]) . Perfect, that worked like a charme! Coming up I will show you how to take this notebook and turn it into a little web app. So stay tuned! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/27/Prepare-Notebook-for-App-Part3.html",
            "relUrl": "/2020/09/27/Prepare-Notebook-for-App-Part3.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Use pretrained BERT model for classification",
            "content": "Would Lasse recommend this book? . This is the second part of the three part blogpost on my NLP project. In this blogpost I will show you how to use a pretrained BERT model to finetune a model to predict how I would rate a book based on one page. In the first part I showed how to build the dataset. Now I will show you how to use this data to basically build a model from that. First, let&#39;s get our packages. . !pip install transformers !pip install seaborn . from pathlib import Path import numpy as np import pandas as pd import torch import torch.nn as nn from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report import transformers from transformers import BertTokenizer, BertForSequenceClassification # specify GPU device = torch.device(&quot;cuda&quot;) . We then load the data we got from our images in combination with pytesseract. . Load Data . p = Path.cwd() complete_df = pd.read_csv(p/&#39;datasets/text_df.csv&#39;) complete_df.head() . text title rating . 0 war ein schrecklicher Rückfall eingetreten.In ... | gegendenStrich | 1 | . 1 höchst moralischer Akt, die Welt von einem sol... | derSeewolf | 5 | . 2 deutsches Luder nehmen. Und sollten Sie es dan... | ButchersCrossing | 4 | . 3 müssen.»Sie kamen jetzt in die Vorstadt. Die S... | diePest | 2 | . 4 ins Gesicht, wandte sich von ihrem traurigen A... | diePest | 2 | . We only need the text and my rating. We also need to substract 1 from my rating for indexing purposes for the cross entropy loss function. . df = pd.DataFrame({ &#39;label&#39;: complete_df.iloc[:,2]-1, &#39;text&#39;: complete_df.iloc[:,0] }) df.head() . label text . 0 0 | war ein schrecklicher Rückfall eingetreten.In ... | . 1 4 | höchst moralischer Akt, die Welt von einem sol... | . 2 3 | deutsches Luder nehmen. Und sollten Sie es dan... | . 3 1 | müssen.»Sie kamen jetzt in die Vorstadt. Die S... | . 4 1 | ins Gesicht, wandte sich von ihrem traurigen A... | . # Get the lists of sentences and their labels. sentences = df.text.values labels = df.label.values . Bert Tokenizer . My data is in German. Luckily, the awesome huggingface library provides a crazy amount of pretrained models in languages from all over the world. We first need our tokenizer: . PRE_TRAINED_MODEL_NAME = &#39;bert-base-german-cased&#39; # Load the BERT tokenizer tokenizer = torch.hub.load(&#39;huggingface/pytorch-transformers&#39;, &#39;tokenizer&#39;, PRE_TRAINED_MODEL_NAME) # Download vocabulary from S3 and cache. . Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_master . Let&#39;s look what the tokenizer does to our text sentences: . # Print the original sentence. print(&#39; Original: &#39;, sentences[0]) # Print the sentence split into tokens. print(&#39;Tokenized: &#39;, tokenizer.tokenize(sentences[0])) # Print the sentence mapped to token ids. print(&#39;Token IDs: &#39;, tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0]))) . Original: war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Literatur als zu jener andern, bei der er einen Platzbeanspruchte, den man ihm verweigerte. Seine Sprache war die des wilden Romantismus, voll gewunde—ner Wendungen und übertriebener Vergleiche, undeigentlich erschien d’Aurévilly wie ein Zuchthengstunter diesen Wallachen, die die ultramontanen StalleDem Herzog kamen diese Betrachtungen heimgelegentlichen Wiederlesen einiger Stellen diesesC L ( ii i „ .. ‚]:„„„„ „a.—näepn alwxxr9rl’iﬂ» Tokenized: [&#39;war&#39;, &#39;ein&#39;, &#39;schreck&#39;, &#39;##licher&#39;, &#39;Rück&#39;, &#39;##fall&#39;, &#39;eingetreten&#39;, &#39;.&#39;, &#39;In&#39;, &#39;dem&#39;, &#39;[UNK]&#39;, &#39;verheiratet&#39;, &#39;##en&#39;, &#39;Priester&#39;, &#39;[UNK]&#39;, &#39;wurde&#39;, &#39;das&#39;, &#39;Lob&#39;, &#39;##Christ&#39;, &#39;##i&#39;, &#39;von&#39;, &#39;Barb&#39;, &#39;##ey&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;gesungen&#39;, &#39;;&#39;, &#39;in&#39;, &#39;[UNK]&#39;, &#39;Les&#39;, &#39;##Di&#39;, &#39;##ab&#39;, &#39;##oli&#39;, &#39;##ques&#39;, &#39;[UNK]&#39;, &#39;hatte&#39;, &#39;sich&#39;, &#39;der&#39;, &#39;Verfasser&#39;, &#39;dem&#39;, &#39;Teufel&#39;, &#39;ergeben&#39;, &#39;,&#39;, &#39;den&#39;, &#39;er&#39;, &#39;pri&#39;, &#39;##es&#39;, &#39;;&#39;, &#39;und&#39;, &#39;jetzt&#39;, &#39;erschien&#39;, &#39;der&#39;, &#39;Sad&#39;, &#39;##ismus&#39;, &#39;,&#39;, &#39;dieser&#39;, &#39;Bast&#39;, &#39;##ard&#39;, &#39;des&#39;, &#39;Kathol&#39;, &#39;##izismus&#39;, &#39;,&#39;, &#39;den&#39;, &#39;die&#39;, &#39;Religion&#39;, &#39;in&#39;, &#39;##allen&#39;, &#39;Formen&#39;, &#39;mit&#39;, &#39;Ex&#39;, &#39;##or&#39;, &#39;##zi&#39;, &#39;##sm&#39;, &#39;##en&#39;, &#39;und&#39;, &#39;Schei&#39;, &#39;##ter&#39;, &#39;##haufen&#39;, &#39;##durch&#39;, &#39;alle&#39;, &#39;Jahrhunderte&#39;, &#39;verfolgt&#39;, &#39;hat&#39;, &#39;.&#39;, &#39;Mit&#39;, &#39;Barb&#39;, &#39;##ey&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;nahm&#39;, &#39;die&#39;, &#39;Serie&#39;, &#39;der&#39;, &#39;rel&#39;, &#39;##i&#39;, &#39;[UNK]&#39;, &#39;g&#39;, &#39;##i&#39;, &#39;##ösen&#39;, &#39;Schriftsteller&#39;, &#39;ein&#39;, &#39;Ende&#39;, &#39;.&#39;, &#39;Eigentlich&#39;, &#39;gehörte&#39;, &#39;dieser&#39;, &#39;Par&#39;, &#39;##ia&#39;, &#39;in&#39;, &#39;jeder&#39;, &#39;Hinsicht&#39;, &#39;mehr&#39;, &#39;zur&#39;, &#39;welt&#39;, &#39;##lichen&#39;, &#39;Literatur&#39;, &#39;als&#39;, &#39;zu&#39;, &#39;jener&#39;, &#39;andern&#39;, &#39;,&#39;, &#39;bei&#39;, &#39;der&#39;, &#39;er&#39;, &#39;einen&#39;, &#39;Platz&#39;, &#39;##be&#39;, &#39;##anspruch&#39;, &#39;##te&#39;, &#39;,&#39;, &#39;den&#39;, &#39;man&#39;, &#39;ihm&#39;, &#39;verweigerte&#39;, &#39;.&#39;, &#39;Seine&#39;, &#39;Sprache&#39;, &#39;war&#39;, &#39;die&#39;, &#39;des&#39;, &#39;wild&#39;, &#39;##en&#39;, &#39;Roman&#39;, &#39;##ti&#39;, &#39;##sm&#39;, &#39;##us&#39;, &#39;,&#39;, &#39;voll&#39;, &#39;gew&#39;, &#39;##unde&#39;, &#39;[UNK]&#39;, &#39;ne&#39;, &#39;##r&#39;, &#39;Wend&#39;, &#39;##ungen&#39;, &#39;und&#39;, &#39;übert&#39;, &#39;##riebene&#39;, &#39;##r&#39;, &#39;Vergleich&#39;, &#39;##e&#39;, &#39;,&#39;, &#39;und&#39;, &#39;##eigent&#39;, &#39;##lich&#39;, &#39;erschien&#39;, &#39;d&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;wie&#39;, &#39;ein&#39;, &#39;Zucht&#39;, &#39;##hen&#39;, &#39;##gst&#39;, &#39;##unter&#39;, &#39;diesen&#39;, &#39;Wall&#39;, &#39;##achen&#39;, &#39;,&#39;, &#39;die&#39;, &#39;die&#39;, &#39;u&#39;, &#39;##lt&#39;, &#39;##ram&#39;, &#39;##ont&#39;, &#39;##anen&#39;, &#39;Stall&#39;, &#39;##e&#39;, &#39;##Dem&#39;, &#39;Herzog&#39;, &#39;kamen&#39;, &#39;diese&#39;, &#39;Betrachtung&#39;, &#39;##en&#39;, &#39;heim&#39;, &#39;##gelegen&#39;, &#39;##tlichen&#39;, &#39;Wieder&#39;, &#39;##lesen&#39;, &#39;einiger&#39;, &#39;Stellen&#39;, &#39;dieses&#39;, &#39;##C&#39;, &#39;L&#39;, &#39;(&#39;, &#39;i&#39;, &#39;##i&#39;, &#39;i&#39;, &#39;[UNK]&#39;, &#39;.&#39;, &#39;.&#39;, &#39;[UNK]&#39;, &#39;]&#39;, &#39;:&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;a&#39;, &#39;.&#39;, &#39;[UNK]&#39;, &#39;n&#39;, &#39;##ä&#39;, &#39;##ep&#39;, &#39;##n&#39;, &#39;al&#39;, &#39;##w&#39;, &#39;##xx&#39;, &#39;##r&#39;, &#39;##9&#39;, &#39;##r&#39;, &#39;##l&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;, &#39;[UNK]&#39;] Token IDs: [185, 39, 21387, 766, 1060, 441, 9387, 26914, 173, 128, 2, 5025, 7, 7335, 2, 192, 93, 10929, 17339, 26899, 88, 18304, 8145, 9, 2, 2, 20397, 26968, 50, 2, 4189, 15845, 228, 13078, 11226, 2, 466, 144, 21, 18241, 128, 18649, 4254, 26918, 86, 67, 22074, 16, 26968, 42, 1868, 3368, 21, 16073, 1500, 26918, 534, 16804, 587, 91, 9032, 20438, 26918, 86, 30, 9373, 50, 2700, 7685, 114, 1108, 34, 517, 6694, 7, 42, 11168, 60, 26128, 4912, 987, 16902, 7547, 193, 26914, 304, 18304, 8145, 9, 2, 2, 1995, 30, 4345, 21, 4628, 26899, 2, 111, 26899, 3670, 6425, 39, 926, 26914, 13935, 2374, 534, 1059, 544, 50, 2617, 8110, 380, 252, 3522, 248, 3595, 153, 81, 8310, 19919, 26918, 178, 21, 67, 303, 1361, 165, 4465, 26, 26918, 86, 478, 787, 26792, 26914, 2072, 4247, 185, 30, 91, 24703, 7, 3529, 15099, 6694, 51, 26918, 1352, 397, 1270, 2, 2055, 26900, 16380, 184, 42, 8685, 25630, 26900, 3115, 26897, 26918, 42, 7656, 68, 3368, 9, 2, 2, 246, 39, 17373, 215, 22336, 940, 1377, 5405, 794, 26918, 30, 30, 2118, 362, 1021, 710, 6678, 16993, 26897, 12939, 5996, 3484, 620, 12115, 7, 6488, 10547, 5323, 2261, 18921, 7844, 4812, 1328, 26958, 94, 26954, 46, 26899, 46, 2, 26914, 26914, 2, 26985, 26964, 2, 2, 2, 2, 2, 18, 26914, 2, 53, 26923, 3154, 26898, 1119, 26915, 21591, 26900, 26942, 26900, 26907, 2, 2, 2] . So our bert-base-german-cased tokenizer splits the words into reasonable parts, which correspond to the token ids (input ids). . Tokenize Dataset . Let&#39;s check the length for each sequence and print the max sequence length. . max_len = 0 # For every sentence... for sent in sentences: # Tokenize the text and add `[CLS]` and `[SEP]` tokens. input_ids = tokenizer.encode(sent, add_special_tokens=True) # Update the maximum sentence length. max_len = max(max_len, len(input_ids)) print(&#39;Max sentence length: &#39;, max_len) . Token indices sequence length is longer than the specified maximum sequence length for this model (538 &gt; 512). Running this sequence through the model will result in indexing errors . Max sentence length: 538 . Das Maximum sentence length is 538. However, the maximum sentence length allowed by Bert is 512, so we have to set max_len to 256. Next, we tokenize all of our sentences. . # Tokenize all of the sentences and map the tokens to thier word IDs. input_ids = [] attention_masks = [] # For every sentence... for sent in sentences: # `encode_plus` will: # (1) Tokenize the sentence. # (2) Prepend the `[CLS]` token to the start. # (3) Append the `[SEP]` token to the end. # (4) Map tokens to their IDs. # (5) Pad or truncate the sentence to `max_length` # (6) Create attention masks for [PAD] tokens. encoded_dict = tokenizer.encode_plus( sent, # Sentence to encode. add_special_tokens = True, # Add &#39;[CLS]&#39; and &#39;[SEP]&#39; truncation=True, max_length = 256, # Pad &amp; truncate all sentences. pad_to_max_length = True, #padding=&#39;max_length=256&#39;, return_attention_mask = True, # Construct attn. masks. return_tensors = &#39;pt&#39;, # Return pytorch tensors. ) # Add the encoded sentence to the list. input_ids.append(encoded_dict[&#39;input_ids&#39;]) # And its attention mask (simply differentiates padding from non-padding). attention_masks.append(encoded_dict[&#39;attention_mask&#39;]) # Convert the lists into tensors. input_ids = torch.cat(input_ids, dim=0) attention_masks = torch.cat(attention_masks, dim=0) labels = torch.tensor(labels) . /opt/conda/envs/fastai/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1764: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding=&#39;longest&#39;` to pad to the longest sequence in the batch, or use `padding=&#39;max_length&#39;` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert). warnings.warn( . Training and Validation Set . We then put our data into a pytorch dataset and split the data into training and validation set. . from torch.utils.data import TensorDataset, random_split # Combine the training inputs into a TensorDataset. dataset = TensorDataset(input_ids, attention_masks, labels) # Create a 80-20 train-validation split. # Calculate the number of samples to include in each set. train_size = int(0.8 * len(dataset)) val_size = len(dataset) - train_size # Divide the dataset by randomly selecting samples. train_dataset, val_dataset = random_split(dataset, [train_size, val_size]) print(&#39;{:&gt;5,} training samples&#39;.format(train_size)) print(&#39;{:&gt;5,} validation samples&#39;.format(val_size)) . 75 training samples 19 validation samples . from torch.utils.data import DataLoader, RandomSampler, SequentialSampler # We take a batch size of 16 batch_size = 16 # We&#39;ll take training samples in random order. train_dataloader = DataLoader( train_dataset, sampler = RandomSampler(train_dataset), batch_size = batch_size ) # For validation the order doesn&#39;t matter, so we&#39;ll just read them sequentially. validation_dataloader = DataLoader( val_dataset, sampler = SequentialSampler(val_dataset), batch_size = batch_size ) . After putting the dataset into a dataloader we define how many different classes we&#39;ve got. . n_classes=5 . Get pretrained model . model = BertForSequenceClassification.from_pretrained( &quot;bert-base-german-cased&quot;, # Use the 12-layer BERT model, with an uncased vocab. num_labels = n_classes, # The number of output labels--2 for binary classification. # You can increase this for multi-class tasks. output_attentions = False, # Whether the model returns attentions weights. output_hidden_states = False, # Whether the model returns all hidden-states. ) # Tell pytorch to run this model on the GPU. model.cuda(); . Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . Optimizer and Learning Rate . We use AdamW as our optimizer and use CrossEntropyLoss as our loss function. . from transformers import AdamW . # Note: AdamW is a class from the huggingface library (as opposed to pytorch) # I believe the &#39;W&#39; stands for &#39;Weight Decay fix&quot; optimizer = AdamW(model.parameters(), lr = 2e-4, # args.learning_rate - default is 5e-5, our notebook had 2e-5 eps = 1e-8 # args.adam_epsilon - default is 1e-8. ) . from transformers import get_linear_schedule_with_warmup epochs = 5 # Total number of training steps is [number of batches] x [number of epochs]. # (Note that this is not the same as the number of training samples). total_steps = len(train_dataloader) * epochs # Create the learning rate scheduler. scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 10, # Default value in run_glue.py num_training_steps = total_steps) . loss_fn = nn.CrossEntropyLoss().to(device) . def format_time(elapsed): &#39;&#39;&#39; Takes a time in seconds and returns a string hh:mm:ss &#39;&#39;&#39; # Round to the nearest second. elapsed_rounded = int(round((elapsed))) # Format as hh:mm:ss return str(datetime.timedelta(seconds=elapsed_rounded)) . Train Model . Let&#39;s train our model! . import random import numpy as np import time import datetime seed_val = 42 random.seed(seed_val) np.random.seed(seed_val) torch.manual_seed(seed_val) torch.cuda.manual_seed_all(seed_val) # We&#39;ll store a number of quantities such as training and validation loss, # validation accuracy, and timings. training_stats = [] # Measure the total training time for the whole run. total_t0 = time.time() # For each epoch... for epoch_i in range(0, epochs): # ======================================== # Training # ======================================== # Perform one full pass over the training set. print(&quot;&quot;) print(&#39;======== Epoch {:} / {:} ========&#39;.format(epoch_i + 1, epochs)) print(&#39;Training...&#39;) # Measure how long the training epoch takes. t0 = time.time() # Reset the total loss for this epoch. total_train_loss = 0 model.train() # For each batch of training data... for step, batch in enumerate(train_dataloader): # Progress update every 40 batches. if step % 40 == 0 and not step == 0: # Calculate elapsed time in minutes. elapsed = format_time(time.time() - t0) # Report progress. print(f&#39;Batch:{step} of {len(train_dataloader)}. Elapsed: {elapsed}&#39;) # `batch` contains three pytorch tensors: # [0]: input ids # [1]: attention masks # [2]: labels b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) model.zero_grad() loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) total_train_loss += loss.item() loss.backward() # Clip the norm of the gradients to 1.0. # This is to help prevent the &quot;exploding gradients&quot; problem. torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() # Update the learning rate. scheduler.step() # Calculate the average loss over all of the batches. avg_train_loss = total_train_loss / len(train_dataloader) # Measure how long this epoch took. training_time = format_time(time.time() - t0) print(&quot;&quot;) print(&quot; Average training loss: {0:.2f}&quot;.format(avg_train_loss)) print(&quot; Training epoch took: {:}&quot;.format(training_time)) # ======================================== # Validation # ======================================== # After the completion of each training epoch, measure our performance on # our validation set. print(&quot;&quot;) print(&quot;Running Validation...&quot;) t0 = time.time() # Put the model in evaluation mode--the dropout layers behave differently # during evaluation. model.eval() # Tracking variables total_eval_accuracy = 0 total_eval_loss = 0 nb_eval_steps = 0 # Evaluate data for one epoch for batch in validation_dataloader: b_input_ids = batch[0].to(device) b_input_mask = batch[1].to(device) b_labels = batch[2].to(device) (loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels) # Accumulate the validation loss. total_eval_loss += loss.item() # Calculate the average loss over all of the batches. avg_val_loss = total_eval_loss / len(validation_dataloader) # Measure how long the validation run took. validation_time = format_time(time.time() - t0) print(f&#39;Validation Loss: {avg_val_loss}&#39;) print(f&#39;Validation took: {validation_time}&#39;) # Record all statistics from this epoch. training_stats.append( { &#39;epoch&#39;: epoch_i + 1, &#39;Training Loss&#39;: avg_train_loss, &#39;Valid. Loss&#39;: avg_val_loss, &#39;Training Time&#39;: training_time, &#39;Validation Time&#39;: validation_time } ) print(&quot;&quot;) print(&quot;Training complete!&quot;) print(&quot;Total training took {:} (h:mm:ss)&quot;.format(format_time(time.time()-total_t0))) . ======== Epoch 1 / 5 ======== Training... Average training loss: 1.52 Training epoch took: 0:00:07 Running Validation... Validation Loss: 1.3076387345790863 Validation took: 0:00:01 ======== Epoch 2 / 5 ======== Training... Average training loss: 1.34 Training epoch took: 0:00:07 Running Validation... Validation Loss: 1.1822895407676697 Validation took: 0:00:01 ======== Epoch 3 / 5 ======== Training... Average training loss: 0.85 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.8835422396659851 Validation took: 0:00:01 ======== Epoch 4 / 5 ======== Training... Average training loss: 0.32 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.46347731351852417 Validation took: 0:00:01 ======== Epoch 5 / 5 ======== Training... Average training loss: 0.13 Training epoch took: 0:00:07 Running Validation... Validation Loss: 0.4694706201553345 Validation took: 0:00:01 Training complete! Total training took 0:00:40 (h:mm:ss) . Evaluation . # Display floats with two decimal places. pd.set_option(&#39;precision&#39;, 2) # Create a DataFrame from our training statistics. df_stats = pd.DataFrame(data=training_stats) # Use the &#39;epoch&#39; as the row index. df_stats = df_stats.set_index(&#39;epoch&#39;) # Display the table. df_stats . Training Loss Valid. Loss Training Time Validation Time . epoch . 1 1.52 | 1.31 | 0:00:07 | 0:00:01 | . 2 1.34 | 1.18 | 0:00:07 | 0:00:01 | . 3 0.85 | 0.88 | 0:00:07 | 0:00:01 | . 4 0.32 | 0.46 | 0:00:07 | 0:00:01 | . 5 0.13 | 0.47 | 0:00:07 | 0:00:01 | . import matplotlib.pyplot as plt import seaborn as sns # Use plot styling from seaborn. sns.set(style=&#39;darkgrid&#39;) # Increase the plot size and font size. sns.set(font_scale=1.5) plt.rcParams[&quot;figure.figsize&quot;] = (12,6) # Plot the learning curve. plt.plot(df_stats[&#39;Training Loss&#39;], &#39;b-o&#39;, label=&quot;Training&quot;) plt.plot(df_stats[&#39;Valid. Loss&#39;], &#39;g-o&#39;, label=&quot;Validation&quot;) # Label the plot. plt.title(&quot;Training &amp; Validation Loss&quot;) plt.xlabel(&quot;Epoch&quot;) plt.ylabel(&quot;Loss&quot;) plt.legend() plt.xticks([1, 2, 3, 4]) plt.show() . Look at results . So far, we can see that our model learns. Let&#39;s have a look how the predictions on our trainloader look like: . # Prediction on test set # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in train_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(&#39; DONE.&#39;) . DONE. DONE. DONE. DONE. DONE. . np.argmax(predictions[4], axis=1) . array([0, 4, 4, 0, 4, 3, 3, 3, 4, 3, 4]) . true_labels[4] . array([0, 4, 4, 0, 4, 3, 3, 3, 4, 3, 4]) . Well that looks right. However, this is the data we trained our model on. Way more interesting is the data our model hasn&#39;t trained on: . # Prediction on test set # Put model in evaluation mode model.eval() # Tracking variables predictions , true_labels = [], [] # Predict for batch in validation_dataloader: # Add batch to GPU batch = tuple(t.to(device) for t in batch) # Unpack the inputs from our dataloader b_input_ids, b_input_mask, b_labels = batch with torch.no_grad(): # Forward pass, calculate logit predictions outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask) logits = outputs[0] # Move logits and labels to CPU logits = logits.detach().cpu().numpy() label_ids = b_labels.to(&#39;cpu&#39;).numpy() # Store predictions and true labels predictions.append(logits) true_labels.append(label_ids) print(&#39; DONE.&#39;) . DONE. DONE. . np.argmax(predictions[0], axis=1) . array([4, 3, 2, 1, 4, 0, 4, 1, 0, 1, 3, 4, 3, 4, 1, 4]) . true_labels[0] . array([3, 3, 1, 1, 4, 0, 0, 1, 0, 1, 3, 4, 3, 4, 1, 4]) . Awesome! We have quite some variation in our prediction and most of the time they look pretty good! We save our model: . Save Model . torch.save(model.state_dict(), p/&#39;model/model_5epochs_lr1e-4.pt&#39;) . To further improve the model, I will get more data and then retrain it. However, I think so far the model does a pretty good job in replacing me when it comes to book recommendations. . In the next part I will show you how to use Binder to make a small web application in which we can upload a picture of a page and then make a prediction how many stars I would probably give this book. . So stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/26/Use-pretrained-Bert-model-for-classification-Part2.html",
            "relUrl": "/2020/09/26/Use-pretrained-Bert-model-for-classification-Part2.html",
            "date": " • Sep 26, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Teach Python how to read",
            "content": "Using PIL and pytesseract . This is the first blogpost of a three to four (I haven&#39;t decided yet) part project. The main idea is that I want to create a model which will tell me how much I would like the book, given an image of a page as in input. . In this part, I will show you how to turn a image of text into actual text, using pytesseract. So let&#39;s first get our packages. . Import packages . !apt-get update !apt-get install libleptonica-dev -y !apt-get install tesseract-ocr tesseract-ocr-dev -y !apt-get install libtesseract-dev -y !apt-get install tesseract-ocr-deu . Ign:1 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 InRelease Hit:2 http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64 Release Hit:3 http://security.ubuntu.com/ubuntu xenial-security InRelease Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 InRelease Hit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64 Release Hit:7 http://archive.ubuntu.com/ubuntu xenial InRelease Hit:9 http://archive.ubuntu.com/ubuntu xenial-updates InRelease Hit:10 http://archive.ubuntu.com/ubuntu xenial-backports InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done libleptonica-dev is already the newest version (1.73-1). 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done tesseract-ocr is already the newest version (3.04.01-4). tesseract-ocr-dev is already the newest version (3.04.01-4). 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done libtesseract-dev is already the newest version (3.04.01-4). 0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded. Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: tesseract-ocr-deu 0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded. Need to get 4153 kB of archives. After this operation, 13.4 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu xenial/universe amd64 tesseract-ocr-deu all 3.04.00-1 [4153 kB] Fetched 4153 kB in 1s (2333 kB/s) debconf: delaying package configuration, since apt-utils is not installed Selecting previously unselected package tesseract-ocr-deu. (Reading database ... 18655 files and directories currently installed.) Preparing to unpack .../tesseract-ocr-deu_3.04.00-1_all.deb ... Unpacking tesseract-ocr-deu (3.04.00-1) ... Setting up tesseract-ocr-deu (3.04.00-1) ... . !pip install pytesseract . Requirement already satisfied: pytesseract in /opt/conda/envs/fastai/lib/python3.8/site-packages (0.3.6) Requirement already satisfied: Pillow in /opt/conda/envs/fastai/lib/python3.8/site-packages (from pytesseract) (7.2.0) . from fastai.vision.all import * from fastai.vision.widgets import * import numpy as np from pathlib import Path import pytesseract import re from PIL import Image, ImageFilter import pandas as pd . Define PosixPath to data . I took about 100 images of pages from books I own (all in German). I then put them in an image folder, let&#39;s have a look at the directory. . p = Path.cwd()/&#39;images&#39; . img_paths = [x for x in p.iterdir()] . img_paths[0].parts[5] . &#39;IMG_5123_gegendenStrich.jpg&#39; . Use regex to extract title of book . Next to the text from the image, I would like to extract the title of the book so I can later easily join my ratings to the texts. I therefore use a regex. . title_list = [re.match(&quot;^.* _(.*) ..*$&quot;,img_paths[i].parts[5]).group(1) for i in range(len(img_paths))] . Automatically read in images, transform them and get text . We use pytesseract for extracting the text from the images. To improve the performance I tried a lot of data transformation: cropping, binarizing and a lot more. The only thing that worked for me was to first rotate the image and then use a MedianFilter. . def proc_img(img_path): im1 = Image.open(img_path) im1 = im1.rotate(angle=270, resample=0, expand=10, center=None, translate=None, fillcolor=None) im1 = im1.filter(ImageFilter.MedianFilter) return im1 . All of my input images are from german books, so I need to use lang=&quot;deu&quot;. . def get_text(img): return pytesseract.image_to_string(img, lang=&quot;deu&quot;) . I again use a regular expression to get rid of common mistakes pytesseract does: putting a n somewhere or confusing a s for a 5. . def use_pattern(text): return pattern.sub(lambda m: rep[re.escape(m.group(0))], text) . rep = {&quot; n&quot;: &quot;&quot;, &quot;`&quot;: &quot;&quot;, &#39;%&#39;:&quot;&quot;, &#39;°&#39;: &#39;&#39;, &#39;&amp;&#39;:&#39;&#39;, &#39;‘&#39;:&#39;&#39;, &#39;€&#39;:&#39;e&#39;, &#39;®&#39;:&#39;&#39;, &#39; &#39;: &#39;&#39;, &#39;5&#39;:&#39;s&#39;, &#39;1&#39;:&#39;i&#39;, &#39;_&#39;:&#39;&#39;, &#39;-&#39;:&#39;&#39;} # define desired replacements here # use these three lines to do the replacement rep = dict((re.escape(k), v) for k, v in rep.items()) #Python 3 renamed dict.iteritems to dict.items so use rep.items() for latest versions pattern = re.compile(&quot;|&quot;.join(rep.keys())) . Use tesseract to make image into text . Finally, we use a list comprehension (they&#39;re super useful) to put all of the text into a list of texts. . text_list = [use_pattern(get_text(proc_img(str(img_paths[i])))) for i in range(len(img_paths))] . Combine into Dataframe . And now let&#39;s put that into a pandas dataframe. . d = {&#39;text&#39;:text_list,&#39;title&#39;:title_list} df = pd.DataFrame(d) df.head() . text title . 0 war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Lite—ratur als zu jener andern, bei der er einen Platzbeanspruchte, den... | gegendenStrich | . 1 höchst moralischer Akt, die Welt von einem solchen Ungeheuerzu befreien? Die Menschheit wäre dann besser und glücklicherund das Leben schöner und süßer.Ich dachte lange darüber nach. Schlaﬂos lag ich in meinerKoj e und hielt mir die Fakten vor Augen. Mit Johnson und Leachredete ich während der Nachtwachen, wenn Wolf Larsen unterDeck war. Beide Männer hatten die Hoffnung verloren — Johnson wegen seiner grundlegenden Schwermut, Leach, weil ersich in seinen vergeblichen Kämpfen erschöpft hatte. Aber eines Nachts ergriff er leidenschaftlich meine Hand und sagte:»Ich glaube, Sie sind in Ordnung... | derSeewolf | . 2 deutsches Luder nehmen. Und sollten Sie es dann allzu eilighaben, Mr. Andrews, werden Sie mich wohl von ihr runterziehen müssen.«Andrews wartete, dass die beiden Männer fortritten, undsah ihnen nach, wie sie sich durch das dämmrige Tal entfern—ten und ihre auf und ab wippenden Gestalten mit der dunkleren Schraffur der westlichen Berghänge verschmelzen. Dannaufs Neue überraschte, lenkte die Hände ab und sorgte dafür,dass ihm die eigenen Gesichtszüge fremd vorkamen; er fragtesich, wie er wohl aussah, fragte sich, ob Francine ihn wiederkennen würde, wenn sie ihn jetzt sehen könnte.Seit dem Ab... | ButchersCrossing | . 3 Doktor, das wissen Sie so gut wie ich. Vor hundert Jah—ren hat eine Pestepidemie in Persien alle Bewohner einerStadt getötet und ausgerechnet den Totenwäscher nicht,der nie aufgehört hatte, seine Arbeit zu verrichten.»müssen.»Sie kamen jetzt in die Vorstadt. Die Scheinwerfer beleuchteten die menschenleeren Straßen. Sie hielten an.Vor dem Auto fragte Rieux Tarrou, ob er mitkommenwolle, und der sagte ja. Ein Schimmer vom Himmel er—hellte ihre Gesichter. Rieux lachte plötzlich freundschaftlich.« Sagen Sie, Tarrou, was treibt Sie dazu, sich damit zubefassen? »&lt;&lt; Ich weiß nicht. Meine Moral vie... | diePest | . 4 ins Gesicht, wandte sich von ihrem traurigen Ausdruckangewidert ab, und nachdem er zum hundertsten Maldie Aushängeschilder der gegenüberliegenden Geschäfte, die Werbung für die schon nicht mehr erhält—lichen bekannten Aperitifmarken gelesen hatte, stander auf und lief ziellos durch die gelben Straßen derStadt. Er schleppte sich von einsamen Spaziergängen inCafés und von Cafés in Restaurants und erreichte soden Abend. An einem solchen Abend sah Rieux denJournalisten zögernd vor der Tür eines Cafés stehen. Erschien sich zu entschließen, ging hinein und setzte sichhinten hin. Es war um die Ze... | diePest | . df.text[3] . &#39;Doktor, das wissen Sie so gut wie ich. Vor hundert Jah—ren hat eine Pestepidemie in Persien alle Bewohner einerStadt getötet und ausgerechnet den Totenwäscher nicht,der nie aufgehört hatte, seine Arbeit zu verrichten.»müssen.»Sie kamen jetzt in die Vorstadt. Die Scheinwerfer beleuchteten die menschenleeren Straßen. Sie hielten an.Vor dem Auto fragte Rieux Tarrou, ob er mitkommenwolle, und der sagte ja. Ein Schimmer vom Himmel er—hellte ihre Gesichter. Rieux lachte plötzlich freundschaftlich.« Sagen Sie, Tarrou, was treibt Sie dazu, sich damit zubefassen? »&lt;&lt; Ich weiß nicht. Meine Moral vielleicht.»«Und die wäre? &gt;&gt;«Verständnis.»Tarrou wandte sich dem Haus zu, und Rieux sah erstwieder sein Gesicht, als sie bei dem alten Asthmatikerwaren.&#39; . Looking good! For this project I also need my ratings for each of the books. I use a dictionary and the map function to easily create a column with my ratings. . Use Dictionary to map my ratings . rating_lasse = {&#39;derPate&#39;: 5, &#39;ButchersCrossing&#39;: 4, &#39;derSeewolf&#39;: 5, &#39;JekyllandHyde&#39;: 4, &#39;gegendenStrich&#39;: 1, &#39;FruestueckmitKaengurus&#39;: 5, &#39;HuckleberryFinn&#39;: 4, &#39;diePest&#39;: 2, &#39;HerzderFinsternis&#39;: 3, &#39;derSpieler&#39;: 4} . df[&#39;rating&#39;] = df[&#39;title&#39;].map(rating_lasse) . df.head() . text title rating . 0 war ein schrecklicher Rückfall eingetreten.In dem »verheirateten Priester« wurde das LobChristi von Barbey d’Aurévilly gesungen; in »LesDiaboliques« hatte sich der Verfasser dem Teufel ergeben, den er pries; und jetzt erschien der Sadismus,dieser Bastard des Katholizismus, den die Religion inallen Formen mit Exorzismen und Scheiterhaufendurch alle Jahrhunderte verfolgt hat.Mit Barbey d’Aurévilly nahm die Serie der reli—giösen Schriftsteller ein Ende. Eigentlich gehörte dieser Paria in jeder Hinsicht mehr zur weltlichen Lite—ratur als zu jener andern, bei der er einen Platzbeanspruchte, den... | gegendenStrich | 1 | . 1 höchst moralischer Akt, die Welt von einem solchen Ungeheuerzu befreien? Die Menschheit wäre dann besser und glücklicherund das Leben schöner und süßer.Ich dachte lange darüber nach. Schlaﬂos lag ich in meinerKoj e und hielt mir die Fakten vor Augen. Mit Johnson und Leachredete ich während der Nachtwachen, wenn Wolf Larsen unterDeck war. Beide Männer hatten die Hoffnung verloren — Johnson wegen seiner grundlegenden Schwermut, Leach, weil ersich in seinen vergeblichen Kämpfen erschöpft hatte. Aber eines Nachts ergriff er leidenschaftlich meine Hand und sagte:»Ich glaube, Sie sind in Ordnung... | derSeewolf | 5 | . 2 deutsches Luder nehmen. Und sollten Sie es dann allzu eilighaben, Mr. Andrews, werden Sie mich wohl von ihr runterziehen müssen.«Andrews wartete, dass die beiden Männer fortritten, undsah ihnen nach, wie sie sich durch das dämmrige Tal entfern—ten und ihre auf und ab wippenden Gestalten mit der dunkleren Schraffur der westlichen Berghänge verschmelzen. Dannaufs Neue überraschte, lenkte die Hände ab und sorgte dafür,dass ihm die eigenen Gesichtszüge fremd vorkamen; er fragtesich, wie er wohl aussah, fragte sich, ob Francine ihn wiederkennen würde, wenn sie ihn jetzt sehen könnte.Seit dem Ab... | ButchersCrossing | 4 | . 3 Doktor, das wissen Sie so gut wie ich. Vor hundert Jah—ren hat eine Pestepidemie in Persien alle Bewohner einerStadt getötet und ausgerechnet den Totenwäscher nicht,der nie aufgehört hatte, seine Arbeit zu verrichten.»müssen.»Sie kamen jetzt in die Vorstadt. Die Scheinwerfer beleuchteten die menschenleeren Straßen. Sie hielten an.Vor dem Auto fragte Rieux Tarrou, ob er mitkommenwolle, und der sagte ja. Ein Schimmer vom Himmel er—hellte ihre Gesichter. Rieux lachte plötzlich freundschaftlich.« Sagen Sie, Tarrou, was treibt Sie dazu, sich damit zubefassen? »&lt;&lt; Ich weiß nicht. Meine Moral vie... | diePest | 2 | . 4 ins Gesicht, wandte sich von ihrem traurigen Ausdruckangewidert ab, und nachdem er zum hundertsten Maldie Aushängeschilder der gegenüberliegenden Geschäfte, die Werbung für die schon nicht mehr erhält—lichen bekannten Aperitifmarken gelesen hatte, stander auf und lief ziellos durch die gelben Straßen derStadt. Er schleppte sich von einsamen Spaziergängen inCafés und von Cafés in Restaurants und erreichte soden Abend. An einem solchen Abend sah Rieux denJournalisten zögernd vor der Tür eines Cafés stehen. Erschien sich zu entschließen, ging hinein und setzte sichhinten hin. Es war um die Ze... | diePest | 2 | . And that&#39;s it, let&#39;s save this dataframe and we&#39;re ready to move on to the model training! . df.to_csv(p/&#39;datasets/text_df.csv&#39;, encoding=&#39;utf8&#39;, index=False) . I hope you enjoyed this blogpost and stay tuned for the next one! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/22/Build_Dataset_from_Images-Part1.html",
            "relUrl": "/2020/09/22/Build_Dataset_from_Images-Part1.html",
            "date": " • Sep 22, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "How to classify Lego figures",
            "content": "What better way to impress your significant other? . Build a Lego Classifier with fastai . !pip install kaggle . We want to save the dataset into the folder /notebooks/storage/data/Lego-Classification: . !kaggle datasets download -d ihelon/lego-minifigures-classification -p /notebooks/storage/data/Lego-Classification . Unzip data using Pythons pathlib library . from pathlib import Path p = Path(&#39;/notebooks/storage/data/Lego-Classification&#39;) filename = Path(&#39;/notebooks/storage/data/Lego-Classification/lego-minifigures-classification.zip&#39;) . Just as before, we can use bash commands from within Jupyter Notebook. So let&#39;s do that to unzip our data. -q is quiet mode, -d points to the direction where to unzip the data. Just see how well Pythons pathlib and bash work together! . !unzip -q {str(filename)} -d {str(p/&quot;train&quot;)} . Imports . from fastbook import * from fastai.vision.widgets import * import pandas as pd . Let&#39;s now use fastai&#39;s &quot;get_image_files()&quot; function to see how the unzipped data looks like in our destination path: . fns = get_image_files(p/&quot;train&quot;) fns . (#316) [Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/009.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/010.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/006.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/001.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/011.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/005.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/004.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/013.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/007.jpg&#39;),Path(&#39;/notebooks/storage/data/Lego-Classification/train/jurassic-world/0002/008.jpg&#39;)...] . Remember, we put the data into our directory &#39;/notebooks/storage/data/Lego-Classification&#39;. After having a quick look at our data it looks like the data is stored as follows: first the genre of our image (marvel/jurassic-world), then the classification of the figure (0001/0002 etc.). Within these folders we find many different pictures of that figure (001.jpg/002.jpg and so on). Let&#39;s confirm this by looking at the metadata. . df = pd.read_csv(f&#39;{p}/index.csv&#39;, index_col=0) df.tail(5) . path class_id train-valid . 311 marvel/0014/008.jpg | 28 | valid | . 312 marvel/0014/009.jpg | 28 | valid | . 313 marvel/0014/010.jpg | 28 | valid | . 314 marvel/0014/011.jpg | 28 | valid | . 315 marvel/0014/012.jpg | 28 | valid | . df_metadata = pd.read_csv(f&#39;{p}/metadata.csv&#39;, usecols=[&#39;class_id&#39;, &#39;lego_names&#39;, &#39;minifigure_name&#39;]) df_metadata.head() . class_id lego_names minifigure_name . 0 1 | [&#39;Spider Mech vs. Venom&#39;] | SPIDER-MAN | . 1 2 | [&#39;Spider Mech vs. Venom&#39;] | VENOM | . 2 3 | [&#39;Spider Mech vs. Venom&#39;] | AUNT MAY | . 3 4 | [&#39;Spider Mech vs. Venom&#39;] | GHOST SPIDER | . 4 5 | [&quot;Yoda&#39;s Hut&quot;] | YODA | . Indeed, that&#39;s how this dataset is structured. What we want is a data structure with which fastai&#39;s data block can easily work with. So what we need is something that gives us the filename, the label and a label which data is for training and which one is for validation. Luckily we can get exactly this by combining the meta-data: . datablock_df = pd.merge(df, df_metadata, left_on=&#39;class_id&#39;, right_on=&#39;class_id&#39;).loc[:,[&#39;path&#39;, &#39;class_id&#39;, &#39;minifigure_name&#39;, &#39;train-valid&#39;]] datablock_df[&#39;is_valid&#39;] = datablock_df[&#39;train-valid&#39;]==&#39;valid&#39; datablock_df.head() . path class_id minifigure_name train-valid is_valid . 0 marvel/0001/001.jpg | 1 | SPIDER-MAN | train | False | . 1 marvel/0001/002.jpg | 1 | SPIDER-MAN | valid | True | . 2 marvel/0001/003.jpg | 1 | SPIDER-MAN | train | False | . 3 marvel/0001/004.jpg | 1 | SPIDER-MAN | train | False | . 4 marvel/0001/005.jpg | 1 | SPIDER-MAN | train | False | . fastai gives us a brief overview of what to check before we can make optimal use of the datablock: . what are the types of our inputs and targets? Images and labels. where is the data? In a dataframe. how do we know if a sample is in the training or the validation set? A column of our dataframe. how do we get an image? By looking at the column path. how do we know the label of an image? By looking at the column minifigure_name. do we want to apply a function to a given sample? Yes, we need to resize everything to a given size. do we want to apply a function to a batch after it&#39;s created? Yes, we want data augmentation. . lego_block = DataBlock(blocks=(ImageBlock, CategoryBlock), splitter=ColSplitter(), get_x=lambda x:p/&quot;train&quot;/f&#39;{x[0]}&#39;, get_y=lambda x:x[2], item_tfms=Resize(224), batch_tfms=aug_transforms()) . Now our datablock is called lego_block. See how it perfectly matches together? . Let me briefly explain what the different steps within our lego_block are doing: first we tell the lego_block on what we want to split our data on (the default here is col=&#39;is_valid&#39;), then we simply put our path column (x[0]) and combine it with our path p and the folder &#39;train&#39; in which is is located in. get_y tells the lego_block where to find the labels in our dataset (x[2]), we then make all of our images the same size and apply transformation on them (checkout fastai for more information). . dls = lego_block.dataloaders(datablock_df) dls.show_batch() . Glorious! . fastai tries to make our life easier. This blog is intended to show you guys how to easily and quickly manage to get a great classifier with it. In the upcoming blogs I will try to better explain what is going on behind the scenes. But for now, let&#39;s enjoy how fast we can build our classifier with fastai! . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(20) . epoch train_loss valid_loss error_rate time . 0 | 5.044340 | 7.069784 | 0.980263 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 4.333549 | 5.407881 | 0.960526 | 00:05 | . 1 | 4.193551 | 4.404543 | 0.947368 | 00:04 | . 2 | 3.994802 | 3.605752 | 0.907895 | 00:04 | . 3 | 3.721432 | 2.941005 | 0.802632 | 00:04 | . 4 | 3.428093 | 2.336661 | 0.638158 | 00:05 | . 5 | 3.092875 | 1.841397 | 0.532895 | 00:04 | . 6 | 2.779624 | 1.470707 | 0.434211 | 00:05 | . 7 | 2.484196 | 1.200388 | 0.348684 | 00:05 | . 8 | 2.216630 | 1.011226 | 0.263158 | 00:05 | . 9 | 1.988927 | 0.901224 | 0.236842 | 00:04 | . 10 | 1.794780 | 0.819948 | 0.217105 | 00:05 | . 11 | 1.624516 | 0.756500 | 0.184211 | 00:04 | . 12 | 1.478024 | 0.716581 | 0.157895 | 00:05 | . 13 | 1.354157 | 0.688189 | 0.164474 | 00:04 | . 14 | 1.244102 | 0.673431 | 0.164474 | 00:05 | . 15 | 1.149104 | 0.662224 | 0.164474 | 00:04 | . 16 | 1.062147 | 0.652154 | 0.164474 | 00:04 | . 17 | 0.985224 | 0.654423 | 0.177632 | 00:04 | . 18 | 0.917764 | 0.653068 | 0.177632 | 00:05 | . 19 | 0.858115 | 0.652137 | 0.184211 | 00:04 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . interp.most_confused(min_val=2) . [(&#39;RON WEASLEY&#39;, &#39;HARRY POTTER&#39;, 2), (&#39;SPIDER-MAN&#39;, &#39;FIREFIGHTER&#39;, 2)] . Not to bad I would say. However, seeing an image of Ronald Weasley and predicting it to be Harry Potter - I&#39;m not sure how much this will impress your significant other. On the other hand, Captain America is correctly predicted 100%. . But we can still try to improve our model by unfreezing the weights, to make the model even better. Let&#39;s check this out: . learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 0.061034 | 0.536347 | 0.151316 | 00:05 | . 1 | 0.046480 | 0.631286 | 0.157895 | 00:04 | . 2 | 0.039729 | 0.572698 | 0.157895 | 00:05 | . Then we will unfreeze the parameters and learn at a slightly lower learning rate: . learn.unfreeze() . learn.fit_one_cycle(2, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.014817 | 0.483588 | 0.125000 | 00:05 | . 1 | 0.016834 | 0.425858 | 0.098684 | 00:04 | . Wow! Down to only 10% error rate. I think that&#39;s quite impressive! Let&#39;s see the confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . With these results I am sure you will impress your significant other! . In one of the next posts I will show you how to use Jupyter to easily set up a small Web App with the help of Binder. So stay tuned! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/16/Lego-Classification.html",
            "relUrl": "/2020/09/16/Lego-Classification.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "How to get the kaggle API started",
            "content": "Easy way to download datasets from kaggle from your terminal . In this blog post I would like to show you how to use kaggles api. And not only that, I will show you how to directly use this API from Jupyter. It&#39;s straightforward and shouldn&#39;t take longer than 5 minutes. Let&#39;s get started! . First of all, log in into your kaggle account and go to &quot;Your Account&quot;. Scroll down until you find the section API. Click on &quot;Create New API Token&quot;. . . Next, upload your downloaded token into a directory to which you have access to. In this blogpost, I use my Cloud Working Space on Paperspace (how to easily set up your own working space in Paperspace click here. . . Note however, that kaggle expects your token to be at a specific folder: ~/.kaggle/kaggle.json on Linux, OSX, and other UNIX-based operating systems, and at C: Users&lt;Windows-username.kaggle kaggle.json on Windows. So that&#39;s what we will do next. Just open your Terminal and move your token to the expected folder. . . And that&#39;s it, you can now use the kaggle API! If you (like me) use a remote computing instance, we don&#39;t want other user to possible use our token. We can prevent this by typing: . . Let&#39;s see how the kaggle API works. First of all, we need to pip install the package. . . If you are looking for a specific dataset, you can now use the kaggle API and simply type: . . If you want to download data from this API, you write: . . In that way you can easily access kaggle datasets and make that work even on cloud computing instances. . Stay tuned for the next blogpost! . Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/15/get-kaggle-api-started.html",
            "relUrl": "/2020/09/15/get-kaggle-api-started.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Setup Paperspace",
            "content": "Kostenlose (und bessere?) Alternative zu Google Colab . Die meisten Machine Learning bzw. Deep Learning Projekte will man nicht auf seinem eigenem PC/Laptop rechnen lassen. Dies hat viele gute Gründe. Zum einen hat nicht jeder Laptop eine geeignete Grafikkarte (GPU), auf der sich DeepLearning Modelle berechnen lassen. Außerdem ist das administrieren der verschiedenen Treiber und das Setup der GPU für Jupyter Notebooks nichts, was super einfach ist. Im Gegenteil. Zudem kostet eine gute Grafikkarte viel Geld. Alles Gründe, weswegen man sich lieber nach kostengünstigen Alternativen umsehen sollte, die einem den administrativen Aufwand abnehmen. . Eine solche Alternative ist Paperspace. Paperspace Gradient stellt ähnlich zu Google Colab eine kostenlose Alternative, wie sich in der Cloud Rechner mit GPUs benutzen lassen. Außerdem hat Paperspace Gradient den Vorteil, dass sich auch Daten wie Datensätze, Bilder, Modelle kostenlos speichern lassen, und nicht nach beenden der Instanz gelöscht werden (wie zum Beispiel in Google Colab). . In diesem Blog will ich zeigen, wie sich über Paperspace eine kostenlose Instanz anlegen lässt, die auch mit GPUs läuft, und wie sich die erstellten Notebooks einfach mit einem bestehenden github Repo verbinden lassen. . Zunächst muss man sich auf der Paperspace-Seite registrieren. Nach der einmaligen Registrierung öffnet sich Folgendes: . . Hier muss zunächst der Container gewählt werden. Das bedeutet nichts anderes, als dass man ein für ein bestimmtes Projekt benötigtes Grund-Setup schon für einen vorinstalliert bekommt - wie wunderbar! Man muss sich über keine Abhängigkeiten kümmern, ist sich sicher, dass die Packages auf dem aktuellsten Stand sind und falls benötigt kann man später immer noch selbst Packages hinzufügen. Was für ein Luxus! Wenn man z.B. ein Projekt mit PyTorch bauen will, so klickt man einfach auf den PyTorch Container. . . Als nächstes wählt man die Maschine, auf der das Projekt laufen soll. Das Beste hierbei: es gibt Maschinen, die von Paperspace komplett umsonst zur Verfügung gestellt werden, sogar mit GPU! Diese sind die letzten 3 Auswahlmöglichkeiten. Und wenn man mehr Power braucht, so kann man auch im Nachhinein noch aufrüsten. . Des weiteren kann man ganz einfach ein bestehendes eigenes git-Repo mit der Instanz verbinden. Dafür einfach den git-Link zum Repo einfügen. . . Zuletzt muss noch eine gültige Kreditkarte hinterlegt werden (wie schon beschrieben, die Kreditkarte wird nur belastet, wenn eine der nicht kotenlosen Alternativen gewählt wird). Danach kann auf &quot;Create new Instance&quot; geklickt werden und schon geht es los! . . Jetzt kann auf &quot;Open&quot; geklickt werden und schon startet sich ein Jupyter Notebook. Das Ganze sollte dann in etwa so aussehen (das README.md stammt schon aus dem verlinkten github-Repo): . . Wie erwähnt ist mein github Repo schon mit der Instanz verbunden. So sieht bislang mein neu angelegtes Repo in github aus: . . Dann lasst uns ein neues Jupyter Notebook anlegen. Einfach auf &quot;New&quot; -&gt; &quot;Python 3&quot; klicken, und es öffnet sich ein Notebook: . . Als nächstes will ich zeigen, wie sich mit Hilfe des Terminals ganz einfach neue packages installieren lassen (über pip), und wie sich das bestehende github Repo aus dem Terminal ansteuern lässt. Dafür muss man bei Jupyter einfach auf &quot;New&quot; und dann -&gt; &quot;Terminal&quot; klicken. . . Mit dem Befehl installieren wir aus fastai das fastbook. Dies funktioniert über pip install. . Jetzt will ich zeigen, wie sich das Terminal ganz einfach mit git verbinden lässt. Über git clone können wir öffentliche git-Repos direkt auf unsere Instanz klonen/kopieren: . . Dies finden wir auch direkt in Jupyter wieder: . . Jetzt wollen wir das Ganze in unser mit der Instanz verbundenem github Repo laden. Dies funktioniert über git add, git commit und git push im Terminal. . . Anschließend muss noch der User Name zum Repo angegeben werden und das dazugehörige Passwort. Das war&#39;s schon. Wie großartig ist das? So sieht das geupdatete github Repo aus: . . Als weitere großartige Möglichkeit bietet Paperspace in den extra dedizierten Ordnern &quot;datasets&quot; und &quot;storage&quot; an, eigene Dateien umsonst in Paperspace zu speichern. Das bedeutet, dass sich bei Neustarten der Instanzen oder neu anlegen der Instanzen diese Dateien immer da sind! Diese Funktionalität bietet Google Colab zum Beispiel nicht an. . Ich hoffe dieser Blog war hilfreich und ihr seid wenigstens halb so begeistert wie ich es bin. . Bleibt dran für die nächsten Posts! . Euer Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/12/Setup-Paperspace.html",
            "relUrl": "/2020/09/12/Setup-Paperspace.html",
            "date": " • Sep 12, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Was ist Gradient Descent und wie funktioniert es?",
            "content": "Eine Coding Anleitung . (dieser Beitrag basiert lose auf dem von fastai zur Verfügung gestellten fastbook) . Dieser Blog Post ist nicht dafür gedacht aufzuzeigen, wie die mathematische Berechnung hinter Gradient Descent funktioniert. Denn zum Glück gibt es genau dafür Computer, die das Ganze wahrscheinlich millionenfach schneller berechnen können als wir. Ich will eine intuitive Erklärung für Gradient Descent liefern, wofür wir Gradient Descent überhaupt brauchen und wie wir mit simplen Python Code unseren eigenen auf Gradient Descent beruhenden Algorithmus bauen können. . Ich erinnere mich, dass ich zu Schulzeiten (und mehr als Abi-Mathe wird für diesen Post wahrhaftig nicht gebraucht) die Ableitungsregeln gelernt und auf Funktionen angewandt habe, aber wirklich Sinn und Zweck habe ich in dem Ganzen nie gesehen. Im besten Fall hat sich die Lehrerin einen an den Haaren herbeigezogenen Fall ausgedacht, um nicht einfach stumpf die Funktion zu liefern, die abgeleitet werden soll. Ich hoffe nach diesem Blog wird klarer, weswegen Ableitungen bzw. die dazugehörigen Gradienten doch eine ziemlich coole und nützliche Sache sind. . Zunächst einmal möchte ich zeigen, was Gradient Descent kann: . . Ein Algorithmus, der erkennen kann, was für ein Bär auf einem gegebenem Bild zu sehen ist? Was zur Hölle hat das mit Gradient Descent zu tun? Wenn du es bis zum Ende des Posts aushältst, wirst du das hoffentlich verstehen. . Gradient Descent hilft Computern dabei zu lernen. Das Ganze sollten wir uns vielleicht in einem Schaubild einmal näher anschauen: . . Die Idee ist dabei wie folgt: Wir haben ein Modell, welches anhand von Bildern erkennen soll, welche Art von Bär hier zu sehen ist. Nachdem wir genügend Bilder von verschiedenen Bären gesammelt haben, fangen wir an unser Modell zu fragen, was es glaubt für eine Art von Bären zu sehen (predict). Wir können uns nun Metriken überlegen, an denen wir erkennen können, wie gut diese Vorhersage unseres Modells ist, zum Beispiel wie häufig das Modell die richtige Bärenart vorhergesagt hat und wie häufig es falsch lag. Wir können daraus einen sogenannten Loss berechnen, was nichts anderes ist als das, was die Lehrer uns immer als Funktion beschrieben haben. Wir haben also eine Funktion die uns angibt, wie gut/schlecht unser Modell Bärenarten vorhersagen kann. Diese Funktion kann zum Beispiel eine simple quadratische Funktion sein, sie kann aber theoretisch jede nur erdenkliche Form annehmen. . Wir wollen unseren Loss minimieren, sprich unsere Funktion soll so gut wie es nur kann die Funktion lernen, wie es möglichst gut Bärenarten voneinander unterscheiden kann. Lass uns eine Sekunde darüber nachdenken. Wir benutzen eine Loss-Funktion, damit wir unserem Modell Feedback geben, wie gut/schlecht es die bisherige Aufgabe gelöst hat. Mithilfe dieser Loss-Funktion lernt unser Modell, die verschiedenen Bärenarten besser zu unterscheiden. Doch wie &quot;lernt&quot; das Modell? Hier kommt Gradient Descent ins Spiel. . Der Gradient, also die Ableitung, gibt uns an, um wie viel die Funktion größer wird, wenn wir (optisch gesprochen) einen kleinen Schritt nach rechts bzw. einen kleinen Schritt nach links gehen von dem Punkt, an dem wir uns gerade befinden. Dies ist die Steigung von dem Punkt, an welchem wir uns gerade befinden. Doch was bringt uns das? . Ich hoffe, du hast einen Moment darüber nachgedacht. Was wir wollen, ist möglichst gut die Bärenarten voneinander zu unterscheiden. Dies steuern wir über unsere Loss-Funktion. Und je geringer unsere Loss-Funktion, desto besser sind wir im Vorhersagen, was für eine Bärenart wir hier gerade haben. Durch den Gradienten wissen wir, in welche Richtung wir uns bewegen müssen, um unsere Loss-Funktion kleiner zu machen. . Noch mag das Ganze recht abstrakt klingen, schon in Kürze folgt hier das Beispiel in Python. Ich will das Ganze aber noch einmal zusammenfassen. Wir wollen etwas optimieren, zum Beispiel möglichst genau die Bärenart vorhersagen. Um diese Funktion zu optimieren, brauchen wir die Loss-Funktion, die uns angibt, wie gut/schlecht unser Modell/unsere Zielfunktion performt. Diese Funktionen können jegliche erdenkliche Formen annehmen. Dem Gradienten ist dies jedoch egal. Der Gradient kann uns zu jedem Ort, an welchem wir uns in der Funktion befinden sagen, was mit unser Loss-Funktion passiert, wenn wir uns ein kleines Stück in eine beliebeige Richtung bewegen. Dies nutzen wir als Feedback, um die Parameter der Zielfunktion so anzupassen, dass die Loss-Funktion kleiner wird, wir also besser die Bärenarten voneinander unterscheiden können. . Dies sind die Schritte, die in dem Schaubild erklärt sind: wir initialisieren die Werte unserer Zielfunktion (anfangs zufällig, weil wir nicht wissen, wie die richtige Funktion aussieht), wir lassen unser Modell Vorhersagen treffen, berechnen daraufhin den Loss und die dazugehörigen Gradienten, um dann durch die Gradienten die Parameter des Modells anzupassen. Diesen Prozess wiederholen wir solange, bis wir mit dem Endergebnis zufrieden sind .Dies sollte auf einem sogenannten Validierungs-Set festgelegt werden, also auf Bildern von Bären, die unser Modell im Trainingsloop nicht sieht. Vielleicht wäre es hier angebracht einmal darüber nachzudenken, warum wir nicht einfach den Trainingsloop solange wiederholen, bis wir alle Bärenarten durch Gradient Descent korrekt vorhersagen können (Stichwort: Overfitting). . Genug geredet, jetzt wollen wir das Ganze auch in Code sehen! . Code Beispiel . Wir wollen mit Hilfe des oben beschriebenen Prozesses eine Funktion finden, die möglichst genau den Verlauf folgender Funktion beschrieben kann: . import torch import matplotlib.pyplot as plt . x = torch.arange(-5,20).float(); x y = 0.75*(x-4)**2 + 0.5*x + 1 plt.scatter(x,y); . Was wir wollen ist die Parameter dieser Funktion zu schätzen. Vom ansehen der Daten können wir bereits auf die funktionale Form schließen - ein Polynom 3ten Grades. Was wir an diesem Beispiel schon erkennen können ist, dass die angenommene Zielfunktion eine große Rolle spielt. Hätten wir von den Daten auf eine quadratische Funktion geschlossen, würden wir die &quot;wahre&quot; Form der Funktion nie richtig bestimmen können. Deep Learning überkommt dieses &quot;Problem&quot;, indem es jede nur erdenkliche Funktion annehmen kann (dazu mehr in einem späteren Post). . def f(x, params): a,b,c,d = params return a*(x-b)**2 + (c*x) + d . Jetzt benötigen wir noch unsere Loss-Funktion (und ich hoffe hier wird klar, wie Loss-Funktion und Zielfunktion miteinander &quot;kommunizieren&quot;): . def mse(preds, targets): return ((preds-targets)**2).mean() . params = torch.randn(4).requires_grad_() params . tensor([ 1.1514, 0.2004, -0.8726, 0.5281], requires_grad=True) . Wir starten unsere Vorhersagen: . preds = f(x, params) plt.scatter(x,preds.detach().numpy()) . &lt;matplotlib.collections.PathCollection at 0x7fd49eb1e580&gt; . Wie gut sehen unsere Vorhersagen aus? . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(x, y) ax.scatter(x, preds.detach().numpy(), color=&#39;red&#39;) ax.set_ylim(-10,100) ax.set_xlim(-5,20) . show_preds(preds) . Wow! Ziemlich miserabel. Wie miserabel zeigt uns unser loss. . loss = mse(preds, y) loss . tensor(8606.2266, grad_fn=&lt;MeanBackward0&gt;) . Auf geht&#39;s Gradient! Zeig uns, wie wir unsere Parameter updaten müssen, damit der Loss kleiner wird. . loss.backward() params.grad . tensor([26820.2754, -4137.0151, 1819.5073, 114.5496]) . Dann lass uns unsere Paramter anpassen (wir multiplizieren den Gradienten mit der sogenannten Learning Rate, dazu in einem weiteren Blog Posts mehr). . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None params . tensor([ 0.8832, 0.2418, -0.8908, 0.5269], requires_grad=True) . Ist unser Loss geringer geworden? . preds = f(x,params) mse(preds, y) . tensor(2857.6997, grad_fn=&lt;MeanBackward0&gt;) . show_preds(preds) . Zum Glück ja. . Wir wollen die steps wiederholen, sodass wir langsam und mithilfe von Gradient Descent unsere Zielfunktion finden. . def apply_step(params, prn=True): preds = f(x, params) loss = mse(preds, y) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds lr=1e-5 for i in range(20): apply_step(params) . 204.3289794921875 201.9536590576172 201.0904083251953 200.72898864746094 200.5342254638672 200.39480590820312 200.27386474609375 200.1591033935547 200.0463104248047 199.9343719482422 199.82254028320312 199.71095275878906 199.59934997558594 199.4879150390625 199.3763885498047 199.26504516601562 199.1535186767578 199.04220581054688 198.93089294433594 198.8196258544922 . _,axs = plt.subplots(1,6,figsize=(24,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Wie man sieht, passt sich die rote Kurve, also unsere Vorhersagen, immer mehr der wahren Kurve an. Alles aufgrund von Gradient Descent! Und genau diese Technik und diese Schritte, die hier in diesem Blog Post aufgezeigt wurden, sind auch die Schritte, die dabei helfen, Neuronale Netze zu trainieren. Die dann wiederum Bären auseinander halten können. . Ich hoffe, durch diesen Post ist die Idee hinter Gradient Descent ein wenig greifbarer geworden und der Sinn und Zweck von Ableitungen könnte von einer anderen Seite vielleicht ein wenig verständlicher betrachtet werden. . Bleibt dran für die nächsten Posts! . Euer Lasse .",
            "url": "https://lschmiddey.github.io/fastpages_/2020/09/01/Gradient-Descent.html",
            "relUrl": "/2020/09/01/Gradient-Descent.html",
            "date": " • Sep 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Ich bin Lasse, ein Data Science Enthusiast! .",
          "url": "https://lschmiddey.github.io/fastpages_/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lschmiddey.github.io/fastpages_/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}